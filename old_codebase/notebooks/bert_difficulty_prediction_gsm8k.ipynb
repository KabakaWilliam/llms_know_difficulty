{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ab41c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f1920b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle(\"dataset/processed/E2H-GSM8K/train.pkl\")\n",
    "test_df = pd.read_pickle(\"dataset/processed/E2H-GSM8K/test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6c05308",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prompts = train_df[\"formatted_prompt\"].tolist()\n",
    "train_difficulty_labels = train_df[\"difficulty\"].tolist()\n",
    "\n",
    "test_prompts = test_df[\"formatted_prompt\"].tolist()\n",
    "test_difficulty_labels = test_df[\"difficulty\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51deb3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31552899",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"allenai/scibert_scivocab_uncased\", num_labels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c99396ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 — train MSE: 0.086490\n",
      "Epoch 2/3 — train MSE: 0.059983\n",
      "Epoch 3/3 — train MSE: 0.055797\n"
     ]
    }
   ],
   "source": [
    "# Fine-tune the sequence classification model to predict difficulty.\n",
    "# Uses existing variables: tokenizer, model, train_prompts, train_difficulty_labels, test_prompts, test_difficulty_labels\n",
    "\n",
    "\n",
    "class PromptDifficultyDataset(Dataset):\n",
    "    def __init__(self, prompts, labels):\n",
    "        self.prompts = prompts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.prompts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.prompts[idx], float(self.labels[idx])\n",
    "\n",
    "def collate_batch(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    # Explicitly set max_length to 512 to match BERT's position embeddings\n",
    "    enc = tokenizer(list(texts), padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    return enc, torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "train_ds = PromptDifficultyDataset(train_prompts, train_difficulty_labels)\n",
    "test_ds  = PromptDifficultyDataset(test_prompts, test_difficulty_labels)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True, collate_fn=collate_batch)\n",
    "test_loader  = DataLoader(test_ds, batch_size=32, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "# Use a lower learning rate for full fine-tuning\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for enc, labels in train_loader:\n",
    "        input_ids = enc[\"input_ids\"].to(device)\n",
    "        attention_mask = enc[\"attention_mask\"].to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass with labels calculates loss automatically\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * input_ids.size(0)\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} — train MSE: {avg_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f88c32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 0.050905, Spearman r: 0.2482\n",
      "pred=0.3796  label=0.9514  prompt_snippet='Please take your time to thoroughly analyze and solve the following math problem'\n",
      "pred=0.3420  label=0.7326  prompt_snippet='Please take your time to thoroughly analyze and solve the following math problem'\n",
      "pred=0.3612  label=0.2413  prompt_snippet='Please take your time to thoroughly analyze and solve the following math problem'\n",
      "pred=0.3442  label=0.4034  prompt_snippet='Please take your time to thoroughly analyze and solve the following math problem'\n",
      "pred=0.3667  label=0.3411  prompt_snippet='Please take your time to thoroughly analyze and solve the following math problem'\n",
      "pred=0.3405  label=0.4756  prompt_snippet='Please take your time to thoroughly analyze and solve the following math problem'\n",
      "pred=0.4875  label=0.1698  prompt_snippet='Please take your time to thoroughly analyze and solve the following math problem'\n",
      "pred=0.4741  label=0.3773  prompt_snippet='Please take your time to thoroughly analyze and solve the following math problem'\n",
      "pred=0.3631  label=0.5969  prompt_snippet='Please take your time to thoroughly analyze and solve the following math problem'\n",
      "pred=0.4320  label=0.4172  prompt_snippet='Please take your time to thoroughly analyze and solve the following math problem'\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for enc, labels in test_loader:\n",
    "        input_ids = enc[\"input_ids\"].to(device)\n",
    "        attention_mask = enc[\"attention_mask\"].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Logits are (batch_size, 1) for regression\n",
    "        preds = outputs.logits.squeeze(-1).cpu()\n",
    "        \n",
    "        all_preds.append(preds)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "all_preds = torch.cat(all_preds).numpy()\n",
    "all_labels = torch.cat(all_labels).numpy()\n",
    "\n",
    "mse = ((all_preds - all_labels) ** 2).mean()\n",
    "# Spearman correlation\n",
    "spearman_corr = pd.Series(all_preds).corr(pd.Series(all_labels), method='spearman')\n",
    "\n",
    "print(f\"Test MSE: {mse:.6f}, Spearman r: {spearman_corr:.4f}\")\n",
    "\n",
    "# Show a few example predictions\n",
    "for i in range(10):\n",
    "    print(f\"pred={all_preds[i]:.4f}  label={all_labels[i]:.4f}  prompt_snippet={test_prompts[i][:80]!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f888d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test MSE: 0.060407, Spearman r: 0.2735"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abb8583",
   "metadata": {},
   "source": [
    "## Using Custom Linear Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "03ac28d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading model for custom regressor training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training custom linear regressor head...\n",
      "Epoch 1/3 — train MSE: 0.054990\n",
      "Epoch 2/3 — train MSE: 0.049645\n",
      "Epoch 3/3 — train MSE: 0.048719\n",
      "Custom Regressor Test MSE: 0.059152, Spearman r: 0.1248\n"
     ]
    }
   ],
   "source": [
    "# Reload model to start from pretrained weights (not fine-tuned)\n",
    "print(\"Reloading model for custom regressor training...\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "model.to(device)\n",
    "\n",
    "# Freeze BERT encoder to train only the regression head\n",
    "for p in model.bert.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "hidden_size = model.config.hidden_size\n",
    "regressor = torch.nn.Linear(hidden_size, 1).to(device)\n",
    "optimizer = torch.optim.AdamW(regressor.parameters(), lr=1e-3)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "epochs = 3\n",
    "print(\"Training custom linear regressor head...\")\n",
    "for epoch in range(epochs):\n",
    "    regressor.train()\n",
    "    running_loss = 0.0\n",
    "    for enc, labels in train_loader:\n",
    "        input_ids = enc[\"input_ids\"].to(device)\n",
    "        attention_mask = enc[\"attention_mask\"].to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            bert_out = model.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            pooled = bert_out.pooler_output\n",
    "\n",
    "        preds = regressor(pooled).squeeze(-1)\n",
    "        loss = loss_fn(preds, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * input_ids.size(0)\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} — train MSE: {avg_loss:.6f}\")\n",
    "\n",
    "# Evaluate\n",
    "regressor.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for enc, labels in test_loader:\n",
    "        input_ids = enc[\"input_ids\"].to(device)\n",
    "        attention_mask = enc[\"attention_mask\"].to(device)\n",
    "        \n",
    "        bert_out = model.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled = bert_out.pooler_output\n",
    "        preds = regressor(pooled).squeeze(-1).cpu()\n",
    "        \n",
    "        all_preds.append(preds)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "all_preds = torch.cat(all_preds).numpy()\n",
    "all_labels = torch.cat(all_labels).numpy()\n",
    "\n",
    "mse = ((all_preds - all_labels) ** 2).mean()\n",
    "spearman_corr = pd.Series(all_preds).corr(pd.Series(all_labels), method='spearman')\n",
    "\n",
    "print(f\"Custom Regressor Test MSE: {mse:.6f}, Spearman r: {spearman_corr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4fd31f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading model for middle layer training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training linear regressor on layer 6 output...\n",
      "Epoch 1/3 — train MSE: 0.050472\n",
      "Epoch 2/3 — train MSE: 0.048330\n",
      "Epoch 3/3 — train MSE: 0.049178\n",
      "Middle Layer (6) Regressor Test MSE: 0.053621, Spearman r: 0.1982\n"
     ]
    }
   ],
   "source": [
    "# Train on Middle Layer (Layer 6)\n",
    "print(\"Reloading model for middle layer training...\")\n",
    "# Ensure output_hidden_states=True to access intermediate layers\n",
    "model = BertForSequenceClassification.from_pretrained(\"allenai/scibert_scivocab_uncased\", output_hidden_states=True)\n",
    "model.to(device)\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# Layer 6 output (index 6 in hidden_states tuple corresponds to output of 6th layer, index 0 is embeddings)\n",
    "target_layer_idx = 6 \n",
    "\n",
    "regressor_mid = torch.nn.Linear(model.config.hidden_size, 1).to(device)\n",
    "optimizer = torch.optim.AdamW(regressor_mid.parameters(), lr=1e-3)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "epochs = 3\n",
    "print(f\"Training linear regressor on layer {target_layer_idx} output...\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    regressor_mid.train()\n",
    "    running_loss = 0.0\n",
    "    for enc, labels in train_loader:\n",
    "        input_ids = enc[\"input_ids\"].to(device)\n",
    "        attention_mask = enc[\"attention_mask\"].to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            # hidden_states is a tuple: (embeddings, layer_1, ..., layer_12)\n",
    "            hidden_states = outputs.hidden_states\n",
    "            # Shape: (batch, seq_len, hidden_size)\n",
    "            layer_output = hidden_states[target_layer_idx]\n",
    "            # CLS pooling (first token)\n",
    "            pooled_output = layer_output[:, 0, :]\n",
    "\n",
    "        preds = regressor_mid(pooled_output).squeeze(-1)\n",
    "        loss = loss_fn(preds, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * input_ids.size(0)\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} — train MSE: {avg_loss:.6f}\")\n",
    "\n",
    "# Evaluate\n",
    "regressor_mid.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for enc, labels in test_loader:\n",
    "        input_ids = enc[\"input_ids\"].to(device)\n",
    "        attention_mask = enc[\"attention_mask\"].to(device)\n",
    "        \n",
    "        outputs = model.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = outputs.hidden_states\n",
    "        layer_output = hidden_states[target_layer_idx]\n",
    "        pooled_output = layer_output[:, 0, :]\n",
    "        \n",
    "        preds = regressor_mid(pooled_output).squeeze(-1).cpu()\n",
    "        all_preds.append(preds)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "all_preds = torch.cat(all_preds).numpy()\n",
    "all_labels = torch.cat(all_labels).numpy()\n",
    "\n",
    "mse = ((all_preds - all_labels) ** 2).mean()\n",
    "spearman_corr = pd.Series(all_preds).corr(pd.Series(all_labels), method='spearman')\n",
    "\n",
    "print(f\"Middle Layer ({target_layer_idx}) Regressor Test MSE: {mse:.6f}, Spearman r: {spearman_corr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e345f821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading model for middle layer training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training linear regressor on layer 10 output...\n",
      "Epoch 1/3 — train MSE: 0.061182\n",
      "Epoch 2/3 — train MSE: 0.049716\n",
      "Epoch 3/3 — train MSE: 0.052389\n",
      "Middle Layer (10) Regressor Test MSE: 0.091720, Pearson r: 0.2492\n"
     ]
    }
   ],
   "source": [
    "# Train on Middle Layer (Layer 6)\n",
    "print(\"Reloading model for middle layer training...\")\n",
    "# Ensure output_hidden_states=True to access intermediate layers\n",
    "model = BertForSequenceClassification.from_pretrained(\"allenai/scibert_scivocab_uncased\", output_hidden_states=True)\n",
    "model.to(device)\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "\n",
    "target_layer_idx = 10 \n",
    "\n",
    "regressor_mid = torch.nn.Linear(model.config.hidden_size, 1).to(device)\n",
    "optimizer = torch.optim.AdamW(regressor_mid.parameters(), lr=1e-3)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "epochs = 3\n",
    "print(f\"Training linear regressor on layer {target_layer_idx} output...\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    regressor_mid.train()\n",
    "    running_loss = 0.0\n",
    "    for enc, labels in train_loader:\n",
    "        input_ids = enc[\"input_ids\"].to(device)\n",
    "        attention_mask = enc[\"attention_mask\"].to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            # hidden_states is a tuple: (embeddings, layer_1, ..., layer_12)\n",
    "            hidden_states = outputs.hidden_states\n",
    "            # Shape: (batch, seq_len, hidden_size)\n",
    "            layer_output = hidden_states[target_layer_idx]\n",
    "            # CLS pooling (first token)\n",
    "            pooled_output = layer_output[:, 0, :]\n",
    "\n",
    "        preds = regressor_mid(pooled_output).squeeze(-1)\n",
    "        loss = loss_fn(preds, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * input_ids.size(0)\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} — train MSE: {avg_loss:.6f}\")\n",
    "\n",
    "# Evaluate\n",
    "regressor_mid.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for enc, labels in test_loader:\n",
    "        input_ids = enc[\"input_ids\"].to(device)\n",
    "        attention_mask = enc[\"attention_mask\"].to(device)\n",
    "        \n",
    "        outputs = model.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = outputs.hidden_states\n",
    "        layer_output = hidden_states[target_layer_idx]\n",
    "        pooled_output = layer_output[:, 0, :]\n",
    "        \n",
    "        preds = regressor_mid(pooled_output).squeeze(-1).cpu()\n",
    "        all_preds.append(preds)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "all_preds = torch.cat(all_preds).numpy()\n",
    "all_labels = torch.cat(all_labels).numpy()\n",
    "\n",
    "mse = ((all_preds - all_labels) ** 2).mean()\n",
    "if all_preds.std() > 0 and all_labels.std() > 0:\n",
    "    corr = ((all_preds - all_preds.mean()) * (all_labels - all_labels.mean())).mean() / (all_preds.std() * all_labels.std())\n",
    "else:\n",
    "    corr = float(\"nan\")\n",
    "\n",
    "print(f\"Middle Layer ({target_layer_idx}) Regressor Test MSE: {mse:.6f}, Pearson r: {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "985bf34a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.bert.encoder.layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "98469db4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109920002"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "325dae0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading model for partial fine-tuning (unfreeze early, freeze late)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfreezing embeddings and encoder layers 0 to 5. Freezing layers 6 to 11.\n",
      "Trainable parameters: 67392001 / 109919233 (61.31%)\n",
      "Epoch 1/3 — train MSE: 0.097253\n",
      "Epoch 2/3 — train MSE: 0.057490\n",
      "Epoch 3/3 — train MSE: 0.058718\n",
      "Partial Fine-tuning Test MSE: 0.055544, Spearman r: 0.2537\n"
     ]
    }
   ],
   "source": [
    "# Partial Fine-tuning: Unfreeze earlier layers, freeze higher layers\n",
    "print(\"Reloading model for partial fine-tuning (unfreeze early, freeze late)...\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"allenai/scibert_scivocab_uncased\", num_labels=1)\n",
    "model.to(device)\n",
    "\n",
    "# 1. Freeze all parameters first\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 2. Unfreeze Embeddings\n",
    "for param in model.bert.embeddings.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 3. Unfreeze earlier encoder layers (e.g., 0 to 5) and keep higher layers frozen (e.g., 6 to 11)\n",
    "n_layers = len(model.bert.encoder.layer)\n",
    "split_layer = 6 # Unfreeze 0-5, Freeze 6-11\n",
    "\n",
    "print(f\"Unfreezing embeddings and encoder layers 0 to {split_layer-1}. Freezing layers {split_layer} to {n_layers-1}.\")\n",
    "\n",
    "for i in range(split_layer):\n",
    "    for param in model.bert.encoder.layer[i].parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "# 4. Unfreeze the classifier head and pooler\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model.bert.pooler.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Check trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "all_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable parameters: {trainable_params} / {all_params} ({trainable_params/all_params:.2%})\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-5)\n",
    "\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for enc, labels in train_loader:\n",
    "        input_ids = enc[\"input_ids\"].to(device)\n",
    "        attention_mask = enc[\"attention_mask\"].to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * input_ids.size(0)\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} — train MSE: {avg_loss:.6f}\")\n",
    "\n",
    "# Evaluate\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for enc, labels in test_loader:\n",
    "        input_ids = enc[\"input_ids\"].to(device)\n",
    "        attention_mask = enc[\"attention_mask\"].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        preds = outputs.logits.squeeze(-1).cpu()\n",
    "        \n",
    "        all_preds.append(preds)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "all_preds = torch.cat(all_preds).numpy()\n",
    "all_labels = torch.cat(all_labels).numpy()\n",
    "\n",
    "mse = ((all_preds - all_labels) ** 2).mean()\n",
    "spearman_corr = pd.Series(all_preds).corr(pd.Series(all_labels), method='spearman')\n",
    "\n",
    "print(f\"Partial Fine-tuning Test MSE: {mse:.6f}, Spearman r: {spearman_corr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b7a83632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109919233"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "de3cbbfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109, 919, 233)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "109,919,233"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b904ac6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dfr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
