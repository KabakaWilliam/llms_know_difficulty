{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9d0adf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/envs/dfr/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-08 03:18:12 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 03:18:14,662\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "from math_verify import parse, verify\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8873fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec4d994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_answer(ground_truth, response):\n",
    "    \"\"\"\n",
    "    Verify if a response matches the ground truth.\n",
    "    \n",
    "    Args:\n",
    "        ground_truth: The correct answer (will be parsed with $ delimiters)\n",
    "        response: The model's response to verify\n",
    "        \n",
    "    Returns:\n",
    "        1 if correct, 0 if incorrect\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ans = parse(response)\n",
    "    except Exception:\n",
    "        ans = response\n",
    "    \n",
    "    try:\n",
    "        if verify(parse(f\"${ground_truth}$\"), ans):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "def evaluate_responses(responses, ground_truths):\n",
    "    \"\"\"\n",
    "    Evaluate a list of responses against ground truths.\n",
    "    \n",
    "    Args:\n",
    "        responses: List of model responses\n",
    "        ground_truths: List of correct answers\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (is_correct_list, accuracy_percentage, num_correct, total)\n",
    "    \"\"\"\n",
    "    is_correct = [verify_answer(gt, resp) for gt, resp in zip(ground_truths, responses)]\n",
    "    accuracy = sum(is_correct) / len(is_correct) * 100 if is_correct else 0.0\n",
    "    return is_correct, accuracy, sum(is_correct), len(is_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42cc3a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"Qwen/Qwen2.5-Math-1.5B-Instruct\"\n",
    "MODEL_ALIAS = MODEL_NAME.split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96591ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_test_set_path = \"/VData/linna4335/llms_know_difficult/runs/Qwen2.5-Math-1.5B/datasplits/AIME_2025_predicted_by_predicting_learnability.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42f7517a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(sample_test_set_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f0487c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MATH_PROMPT_FORMATTING = \" Please put your final answer inside \\\\boxed{}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebdd8e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"formatted_prompt\"] = df[\"question\"].apply(lambda x: x + MATH_PROMPT_FORMATTING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66a7948d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-08 03:18:23 [config.py:717] This model supports multiple tasks: {'reward', 'generate', 'classify', 'score', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 12-08 03:18:24 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "INFO 12-08 03:18:24 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "INFO 12-08 03:18:25 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='Qwen/Qwen2.5-Math-1.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-Math-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-Math-1.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "INFO 12-08 03:18:25 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='Qwen/Qwen2.5-Math-1.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-Math-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-Math-1.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 12-08 03:18:27 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f328e9c94e0>\n",
      "WARNING 12-08 03:18:27 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f328e9c94e0>\n",
      "INFO 12-08 03:18:27 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 12-08 03:18:27 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "INFO 12-08 03:18:27 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.\n",
      "INFO 12-08 03:18:27 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 12-08 03:18:27 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "INFO 12-08 03:18:27 [topk_topp_sampler.py:59] Using FlashInfer for top-p & top-k sampling.\n",
      "INFO 12-08 03:18:27 [gpu_model_runner.py:1329] Starting to load model Qwen/Qwen2.5-Math-1.5B-Instruct...\n",
      "INFO 12-08 03:18:27 [gpu_model_runner.py:1329] Starting to load model Qwen/Qwen2.5-Math-1.5B-Instruct...\n",
      "INFO 12-08 03:18:28 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
      "INFO 12-08 03:18:28 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
      "INFO 12-08 03:18:28 [weight_utils.py:315] No model.safetensors.index.json found in remote.\n",
      "INFO 12-08 03:18:28 [weight_utils.py:315] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.66it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.65it/s]\n",
      "\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.66it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.65it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-08 03:18:28 [loader.py:458] Loading weights took 0.41 seconds\n",
      "INFO 12-08 03:18:28 [gpu_model_runner.py:1347] Model loading took 2.8798 GiB and 1.000572 seconds\n",
      "INFO 12-08 03:18:28 [gpu_model_runner.py:1347] Model loading took 2.8798 GiB and 1.000572 seconds\n",
      "INFO 12-08 03:18:34 [backends.py:420] Using cache directory: /home/lina4335/.cache/vllm/torch_compile_cache/0426d30a27/rank_0_0 for vLLM's torch.compile\n",
      "INFO 12-08 03:18:34 [backends.py:430] Dynamo bytecode transform time: 5.46 s\n",
      "INFO 12-08 03:18:34 [backends.py:420] Using cache directory: /home/lina4335/.cache/vllm/torch_compile_cache/0426d30a27/rank_0_0 for vLLM's torch.compile\n",
      "INFO 12-08 03:18:34 [backends.py:430] Dynamo bytecode transform time: 5.46 s\n",
      "INFO 12-08 03:18:37 [backends.py:118] Directly load the compiled graph(s) for shape None from the cache, took 3.077 s\n",
      "INFO 12-08 03:18:37 [backends.py:118] Directly load the compiled graph(s) for shape None from the cache, took 3.077 s\n",
      "INFO 12-08 03:18:38 [monitor.py:33] torch.compile takes 5.46 s in total\n",
      "INFO 12-08 03:18:38 [monitor.py:33] torch.compile takes 5.46 s in total\n",
      "INFO 12-08 03:18:38 [kv_cache_utils.py:634] GPU KV cache size: 969,408 tokens\n",
      "INFO 12-08 03:18:38 [kv_cache_utils.py:637] Maximum concurrency for 4,096 tokens per request: 236.67x\n",
      "INFO 12-08 03:18:38 [kv_cache_utils.py:634] GPU KV cache size: 969,408 tokens\n",
      "INFO 12-08 03:18:38 [kv_cache_utils.py:637] Maximum concurrency for 4,096 tokens per request: 236.67x\n",
      "INFO 12-08 03:18:54 [gpu_model_runner.py:1686] Graph capturing finished in 16 secs, took 0.56 GiB\n",
      "INFO 12-08 03:18:54 [gpu_model_runner.py:1686] Graph capturing finished in 16 secs, took 0.56 GiB\n",
      "INFO 12-08 03:18:54 [core.py:159] init engine (profile, create kv cache, warmup model) took 25.71 seconds\n",
      "INFO 12-08 03:18:54 [core.py:159] init engine (profile, create kv cache, warmup model) took 25.71 seconds\n",
      "INFO 12-08 03:18:54 [core_client.py:439] Core engine process 0 ready.\n",
      "INFO 12-08 03:18:54 [core_client.py:439] Core engine process 0 ready.\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=MODEL_NAME, gpu_memory_utilization=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "187953ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d5a2d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPTS = df[\"formatted_prompt\"].to_list()\n",
    "GTS = df[\"answer\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b341236c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = SamplingParams(temperature=0.0, max_tokens=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5324c016",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/15 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 15/15 [00:04<00:00,  3.06it/s, est. speed input: 358.86 toks/s, output: 2682.63 toks/s]\n",
      "Processed prompts: 100%|██████████| 15/15 [00:04<00:00,  3.06it/s, est. speed input: 358.86 toks/s, output: 2682.63 toks/s]\n"
     ]
    }
   ],
   "source": [
    "outputs = llm.generate(PROMPTS, PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b037d564",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESP = []\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    RESP.append(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f5cace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate responses using reusable function\n",
    "is_correct, accuracy, num_correct, total = evaluate_responses(RESP, GTS)\n",
    "print(f\"Accuracy: {accuracy:.2f}% ({num_correct}/{total})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3c9870a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[70, 588, 16, 117, 279, 504, 821, 77, 62, 81, 259, 510, 204, 60, 735]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bb57a804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c852c3c",
   "metadata": {},
   "source": [
    "## 2-Round Inference on All Questions\n",
    "\n",
    "Now let's perform 2-round inference on all questions. In round 1, we generate an initial answer. In round 2, we ask the model to review and potentially correct its work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad89e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Round 1 responses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 15/15 [00:04<00:00,  3.60it/s, est. speed input: 421.80 toks/s, output: 2929.05 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed Round 1 for 15 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Round 1: Generate initial answers\n",
    "PROMPTS_ROUND1 = df[\"formatted_prompt\"].to_list()\n",
    "GTS_EASY = df[\"answer\"].to_list()\n",
    "\n",
    "print(\"Generating Round 1 responses...\")\n",
    "outputs_round1 = llm.generate(PROMPTS_ROUND1, PARAMS)\n",
    "\n",
    "RESP_ROUND1 = []\n",
    "for output in outputs_round1:\n",
    "    generated_text = output.outputs[0].text\n",
    "    RESP_ROUND1.append(generated_text)\n",
    "    \n",
    "print(f\"Completed Round 1 for {len(RESP_ROUND1)} questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1422b322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd05118f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"/VData/linna4335/llms_know_difficult/hard_rl/data/MATH/train.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1e12ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Round 2 responses (review)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/15 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 15/15 [00:11<00:00,  1.34it/s, est. speed input: 1308.23 toks/s, output: 3399.34 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed Round 2 for 15 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Round 2: Review and correct the work\n",
    "REVIEW_PROMPT_TEMPLATE = \"\"\"Original Question: {question}\n",
    "\n",
    "Your Initial Answer: {initial_answer}\n",
    "\n",
    "Please carefully review your answer above. Check for any calculation errors, logical mistakes, or incorrect reasoning. If you find any errors, provide a corrected answer. If your answer is correct, restate it.\n",
    "\n",
    "Please put your final answer inside \\\\boxed{{}}.\"\"\"\n",
    "\n",
    "PROMPTS_ROUND2 = []\n",
    "for idx, (question, response) in enumerate(zip(df[\"question\"].to_list(), RESP_ROUND1)):\n",
    "    review_prompt = REVIEW_PROMPT_TEMPLATE.format(\n",
    "        question=question,\n",
    "        initial_answer=response\n",
    "    )\n",
    "    PROMPTS_ROUND2.append(review_prompt)\n",
    "\n",
    "print(\"Generating Round 2 responses (review)...\")\n",
    "outputs_round2 = llm.generate(PROMPTS_ROUND2, PARAMS)\n",
    "\n",
    "RESP_ROUND2 = []\n",
    "for output in outputs_round2:\n",
    "    generated_text = output.outputs[0].text\n",
    "    RESP_ROUND2.append(generated_text)\n",
    "    \n",
    "print(f\"Completed Round 2 for {len(RESP_ROUND2)} questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48609a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1 Accuracy: 13.33% (2/15)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Round 1 results\n",
    "is_correct_round1, accuracy_round1, num_correct_r1, total_r1 = evaluate_responses(RESP_ROUND1, GTS_EASY)\n",
    "print(f\"Round 1 Accuracy: {accuracy_round1:.2f}% ({num_correct_r1}/{total_r1})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8db8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 2 Accuracy: 13.33% (2/15)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Round 2 results\n",
    "is_correct_round2, accuracy_round2, num_correct_r2, total_r2 = evaluate_responses(RESP_ROUND2, GTS_EASY)\n",
    "print(f\"Round 2 Accuracy: {accuracy_round2:.2f}% ({num_correct_r2}/{total_r2})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d170a7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 2-Round Inference Results ===\n",
      "Round 1 Accuracy: 13.33%\n",
      "Round 2 Accuracy: 13.33%\n",
      "Improvement: 0.00 percentage points\n",
      "\n",
      "Questions improved by review: 0\n",
      "Questions degraded by review: 0\n",
      "Questions unchanged: 15\n"
     ]
    }
   ],
   "source": [
    "# Compare results and identify improvements\n",
    "results_comparison = []\n",
    "for idx in range(len(is_correct_round1)):\n",
    "    result = {\n",
    "        'question_idx': idx,\n",
    "        'predicted_difficulty': df.iloc[idx]['predicted_difficulty_sigmoid'],\n",
    "        'round1_correct': is_correct_round1[idx],\n",
    "        'round2_correct': is_correct_round2[idx],\n",
    "        'improved': is_correct_round2[idx] > is_correct_round1[idx],\n",
    "        'degraded': is_correct_round2[idx] < is_correct_round1[idx]\n",
    "    }\n",
    "    results_comparison.append(result)\n",
    "\n",
    "comparison_df = pd.DataFrame(results_comparison)\n",
    "\n",
    "# Summary statistics\n",
    "num_improved = comparison_df['improved'].sum()\n",
    "num_degraded = comparison_df['degraded'].sum()\n",
    "num_unchanged = len(comparison_df) - num_improved - num_degraded\n",
    "\n",
    "print(f\"\\n=== 2-Round Inference Results ===\")\n",
    "print(f\"Round 1 Accuracy: {accuracy_round1:.2f}%\")\n",
    "print(f\"Round 2 Accuracy: {accuracy_round2:.2f}%\")\n",
    "print(f\"Improvement: {accuracy_round2 - accuracy_round1:.2f} percentage points\")\n",
    "print(f\"\\nQuestions improved by review: {num_improved}\")\n",
    "print(f\"Questions degraded by review: {num_degraded}\")\n",
    "print(f\"Questions unchanged: {num_unchanged}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "15a27562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_idx</th>\n",
       "      <th>predicted_difficulty</th>\n",
       "      <th>round1_correct</th>\n",
       "      <th>round2_correct</th>\n",
       "      <th>improved</th>\n",
       "      <th>degraded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.085126</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.185826</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.199134</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.224215</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.230331</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.230922</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.244214</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.257724</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.303385</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.331925</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.332469</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.445939</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.495251</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.519142</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0.558149</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    question_idx  predicted_difficulty  round1_correct  round2_correct  \\\n",
       "0              0              0.085126               0               0   \n",
       "1              1              0.185826               0               0   \n",
       "2              2              0.199134               0               0   \n",
       "3              3              0.224215               0               0   \n",
       "4              4              0.230331               0               0   \n",
       "5              5              0.230922               0               0   \n",
       "6              6              0.244214               0               0   \n",
       "7              7              0.257724               0               0   \n",
       "8              8              0.303385               0               0   \n",
       "9              9              0.331925               0               0   \n",
       "10            10              0.332469               1               1   \n",
       "11            11              0.445939               0               0   \n",
       "12            12              0.495251               0               0   \n",
       "13            13              0.519142               1               1   \n",
       "14            14              0.558149               0               0   \n",
       "\n",
       "    improved  degraded  \n",
       "0      False     False  \n",
       "1      False     False  \n",
       "2      False     False  \n",
       "3      False     False  \n",
       "4      False     False  \n",
       "5      False     False  \n",
       "6      False     False  \n",
       "7      False     False  \n",
       "8      False     False  \n",
       "9      False     False  \n",
       "10     False     False  \n",
       "11     False     False  \n",
       "12     False     False  \n",
       "13     False     False  \n",
       "14     False     False  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display comparison dataframe\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b81804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improved answers found.\n"
     ]
    }
   ],
   "source": [
    "# Examine examples where the model improved its answer\n",
    "improved_indices = comparison_df[comparison_df['improved']]['question_idx'].tolist()\n",
    "\n",
    "if improved_indices:\n",
    "    print(f\"=== Example of Improved Answer ===\")\n",
    "    idx = improved_indices[0]\n",
    "    print(f\"\\nQuestion: {df.iloc[idx]['question'][:200]}...\")\n",
    "    print(f\"\\nGround Truth: {GTS_EASY[idx]}\")\n",
    "    print(f\"\\nRound 1 Answer (Incorrect):\\n{RESP_ROUND1[idx][:300]}...\")\n",
    "    print(f\"\\nRound 2 Answer (Correct):\\n{RESP_ROUND2[idx][:300]}...\")\n",
    "else:\n",
    "    print(\"No improved answers found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e29baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No degraded answers found.\n"
     ]
    }
   ],
   "source": [
    "# Examine examples where the model degraded its answer\n",
    "degraded_indices = comparison_df[comparison_df['degraded']]['question_idx'].tolist()\n",
    "\n",
    "if degraded_indices:\n",
    "    print(f\"=== Example of Degraded Answer ===\")\n",
    "    idx = degraded_indices[0]\n",
    "    print(f\"\\nQuestion: {df.iloc[idx]['question'][:200]}...\")\n",
    "    print(f\"\\nGround Truth: {GTS_EASY[idx]}\")\n",
    "    print(f\"\\nRound 1 Answer (Correct):\\n{RESP_ROUND1[idx][:300]}...\")\n",
    "    print(f\"\\nRound 2 Answer (Incorrect):\\n{RESP_ROUND2[idx][:300]}...\")\n",
    "else:\n",
    "    print(\"No degraded answers found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fa0b4d",
   "metadata": {},
   "source": [
    "## Gating to solve harder questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df2f53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select harder questions (high predicted difficulty)\n",
    "df_sorted_hard = df.sort_values('predicted_difficulty_sigmoid', ascending=False)\n",
    "df_hard = df_sorted_hard.head(len(df))  # Adjust to select subset\n",
    "\n",
    "print(f\"Total questions: {len(df)}\")\n",
    "print(f\"Questions selected for hard question strategies: {len(df_hard)}\")\n",
    "print(f\"Predicted difficulty range: {df_hard['predicted_difficulty_sigmoid'].min():.4f} - {df_hard['predicted_difficulty_sigmoid'].max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cf642c",
   "metadata": {},
   "source": [
    "### Strategy 1: Multi-Sample with Majority Vote\n",
    "Generate multiple solutions and take the majority answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9832d79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 1: Generate multiple samples and use majority voting\n",
    "NUM_SAMPLES = 5\n",
    "PARAMS_SAMPLING = SamplingParams(temperature=0.7, max_tokens=3000, top_p=0.95)\n",
    "\n",
    "PROMPTS_HARD = df_hard[\"formatted_prompt\"].to_list()\n",
    "GTS_HARD = df_hard[\"answer\"].to_list()\n",
    "\n",
    "# Replicate each prompt NUM_SAMPLES times\n",
    "prompts_repeated = []\n",
    "question_indices = []\n",
    "for idx, prompt in enumerate(PROMPTS_HARD):\n",
    "    for _ in range(NUM_SAMPLES):\n",
    "        prompts_repeated.append(prompt)\n",
    "        question_indices.append(idx)\n",
    "\n",
    "print(f\"Generating {len(prompts_repeated)} responses ({NUM_SAMPLES} per question)...\")\n",
    "outputs_majority = llm.generate(prompts_repeated, PARAMS_SAMPLING)\n",
    "\n",
    "# Collect responses\n",
    "responses_by_question = [[] for _ in range(len(PROMPTS_HARD))]\n",
    "for output, q_idx in zip(outputs_majority, question_indices):\n",
    "    text = output.outputs[0].text\n",
    "    responses_by_question[q_idx].append(text)\n",
    "\n",
    "print(f\"Generated {NUM_SAMPLES} samples for {len(PROMPTS_HARD)} questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8119ae94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse all responses and find majority answer\n",
    "from collections import Counter\n",
    "\n",
    "majority_answers = []\n",
    "is_correct_majority = []\n",
    "\n",
    "for q_idx, responses in enumerate(responses_by_question):\n",
    "    # Parse all responses for this question\n",
    "    parsed_responses = []\n",
    "    for resp in responses:\n",
    "        try:\n",
    "            ans = parse(resp)\n",
    "        except Exception:\n",
    "            ans = resp\n",
    "        parsed_responses.append(str(ans))  # Convert to string for counting\n",
    "    \n",
    "    # Find majority answer\n",
    "    if parsed_responses:\n",
    "        answer_counts = Counter(parsed_responses)\n",
    "        majority_answer = answer_counts.most_common(1)[0][0]\n",
    "        majority_answers.append(majority_answer)\n",
    "        is_correct_majority.append(verify_answer(GTS_HARD[q_idx], majority_answer))\n",
    "    else:\n",
    "        majority_answers.append(\"\")\n",
    "        is_correct_majority.append(0)\n",
    "\n",
    "accuracy_majority = sum(is_correct_majority) / len(is_correct_majority) * 100\n",
    "print(f\"Strategy 1 - Majority Vote Accuracy: {accuracy_majority:.2f}% ({sum(is_correct_majority)}/{len(is_correct_majority)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fb7f06",
   "metadata": {},
   "source": [
    "### Strategy 2: Chain-of-Thought with Self-Verification\n",
    "Ask model to solve, then verify its own work step-by-step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb35d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 2: Chain-of-thought with self-verification\n",
    "COT_PROMPT_TEMPLATE = \"\"\"{question}\n",
    "\n",
    "Let's approach this step-by-step:\n",
    "1) First, identify what the problem is asking\n",
    "2) Work through the solution carefully\n",
    "3) Check your work by verifying each step\n",
    "4) Provide your final answer\n",
    "\n",
    "Please put your final answer inside \\\\boxed{{}}.\"\"\"\n",
    "\n",
    "PROMPTS_COT = [COT_PROMPT_TEMPLATE.format(question=q) for q in df_hard[\"question\"].to_list()]\n",
    "\n",
    "print(\"Generating Chain-of-Thought responses...\")\n",
    "outputs_cot = llm.generate(PROMPTS_COT, PARAMS)\n",
    "\n",
    "RESP_COT = [output.outputs[0].text for output in outputs_cot]\n",
    "print(f\"Completed CoT generation for {len(RESP_COT)} questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1006f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate CoT results\n",
    "is_correct_cot, accuracy_cot, num_correct_cot, total_cot = evaluate_responses(RESP_COT, GTS_HARD)\n",
    "print(f\"Strategy 2 - Chain-of-Thought Accuracy: {accuracy_cot:.2f}% ({num_correct_cot}/{total_cot})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5580142d",
   "metadata": {},
   "source": [
    "### Strategy 3: Multi-Step Verification with Reflection\n",
    "Generate answer, then ask model to verify and correct if needed (like 2-round but with explicit verification prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641b8771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 3: Multi-step with explicit verification\n",
    "# Step 1: Generate initial solution\n",
    "print(\"Step 1: Generating initial solutions...\")\n",
    "outputs_step1 = llm.generate(PROMPTS_HARD, PARAMS)\n",
    "RESP_STEP1 = [output.outputs[0].text for output in outputs_step1]\n",
    "\n",
    "# Step 2: Ask model to verify and critique its own work\n",
    "VERIFY_PROMPT_TEMPLATE = \"\"\"Original Question: {question}\n",
    "\n",
    "Proposed Solution: {solution}\n",
    "\n",
    "Now, please act as a mathematical reviewer. Carefully verify the solution above:\n",
    "1) Check if all steps are logically sound\n",
    "2) Verify all calculations are correct\n",
    "3) Confirm the final answer makes sense given the problem\n",
    "4) If you find any errors, provide the corrected solution\n",
    "\n",
    "Provide your final verified answer inside \\\\boxed{{}}.\"\"\"\n",
    "\n",
    "PROMPTS_VERIFY = []\n",
    "for q, sol in zip(df_hard[\"question\"].to_list(), RESP_STEP1):\n",
    "    PROMPTS_VERIFY.append(VERIFY_PROMPT_TEMPLATE.format(question=q, solution=sol))\n",
    "\n",
    "print(\"Step 2: Verifying and correcting solutions...\")\n",
    "outputs_verify = llm.generate(PROMPTS_VERIFY, PARAMS)\n",
    "RESP_VERIFIED = [output.outputs[0].text for output in outputs_verify]\n",
    "\n",
    "print(f\"Completed multi-step verification for {len(RESP_VERIFIED)} questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e371db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate verification strategy results\n",
    "is_correct_verified, accuracy_verified, num_correct_verified, total_verified = evaluate_responses(RESP_VERIFIED, GTS_HARD)\n",
    "print(f\"Strategy 3 - Multi-Step Verification Accuracy: {accuracy_verified:.2f}% ({num_correct_verified}/{total_verified})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856e5154",
   "metadata": {},
   "source": [
    "### Strategy 4: Ensemble - Best of All Strategies\n",
    "For each question, use the answer from whichever strategy got it correct (simulating picking the best approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526cf5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 4: Ensemble - use any strategy that got it correct\n",
    "is_correct_ensemble = []\n",
    "for idx in range(len(GTS_HARD)):\n",
    "    # If any strategy got it correct, mark as correct\n",
    "    strategies_correct = [\n",
    "        is_correct_majority[idx],\n",
    "        is_correct_cot[idx],\n",
    "        is_correct_verified[idx]\n",
    "    ]\n",
    "    is_correct_ensemble.append(1 if any(strategies_correct) else 0)\n",
    "\n",
    "accuracy_ensemble = sum(is_correct_ensemble) / len(is_correct_ensemble) * 100\n",
    "print(f\"Strategy 4 - Ensemble (Any Correct) Accuracy: {accuracy_ensemble:.2f}% ({sum(is_correct_ensemble)}/{len(is_correct_ensemble)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ca7f7f",
   "metadata": {},
   "source": [
    "### Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8efbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all strategies\n",
    "results_summary = pd.DataFrame({\n",
    "    'Strategy': ['Majority Vote (5 samples)', 'Chain-of-Thought', 'Multi-Step Verification', 'Ensemble (Any Correct)'],\n",
    "    'Accuracy': [accuracy_majority, accuracy_cot, accuracy_verified, accuracy_ensemble],\n",
    "    'Correct': [sum(is_correct_majority), sum(is_correct_cot), sum(is_correct_verified), sum(is_correct_ensemble)],\n",
    "    'Total': [len(is_correct_majority), len(is_correct_cot), len(is_correct_verified), len(is_correct_ensemble)]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"HARD QUESTIONS STRATEGY COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(results_summary.to_string(index=False))\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72db7a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze which questions each strategy solved\n",
    "strategy_comparison = pd.DataFrame({\n",
    "    'question_idx': range(len(GTS_HARD)),\n",
    "    'predicted_difficulty': df_hard['predicted_difficulty_sigmoid'].values,\n",
    "    'majority_vote': is_correct_majority,\n",
    "    'chain_of_thought': is_correct_cot,\n",
    "    'verification': is_correct_verified,\n",
    "    'any_correct': is_correct_ensemble\n",
    "})\n",
    "\n",
    "# Count unique solutions per strategy\n",
    "strategy_comparison['num_strategies_correct'] = (\n",
    "    strategy_comparison['majority_vote'] + \n",
    "    strategy_comparison['chain_of_thought'] + \n",
    "    strategy_comparison['verification']\n",
    ")\n",
    "\n",
    "print(\"\\nQuestions solved by number of strategies:\")\n",
    "print(strategy_comparison['num_strategies_correct'].value_counts().sort_index())\n",
    "\n",
    "# Show questions that only one strategy solved\n",
    "unique_solutions = strategy_comparison[strategy_comparison['num_strategies_correct'] == 1]\n",
    "if len(unique_solutions) > 0:\n",
    "    print(f\"\\n{len(unique_solutions)} questions solved by only ONE strategy (showing strategy complementarity)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9258c66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7ba10b",
   "metadata": {},
   "source": [
    "### Optional: Using a Stronger Model as Verifier\n",
    "\n",
    "If you have access to a stronger model (e.g., Qwen2.5-Math-7B), you can load it to verify answers from the weaker model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173c87ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "STRONG_MODEL_NAME = \"Qwen/Qwen2.5-Math-7B-Instruct\"\n",
    "llm_strong = LLM(model=STRONG_MODEL_NAME, gpu_memory_utilization=0.9)\n",
    "\n",
    "VERIFIER_PROMPT_TEMPLATE = \"\"\"Question: {question}\n",
    "\n",
    "A student provided this solution:\n",
    "{weak_solution}\n",
    "\n",
    "As an expert mathematician, please:\n",
    "1) Verify if the solution is correct\n",
    "2) If incorrect, provide the correct solution\n",
    "3) If correct but can be improved, provide a clearer version\n",
    "\n",
    "Please put your final answer inside \\\\boxed{{}}.\"\"\"\n",
    "\n",
    "# Use strong model to verify weak model's answers\n",
    "PROMPTS_STRONG_VERIFY = []\n",
    "for q, weak_sol in zip(df_hard[\"question\"].to_list(), RESP_STEP1):\n",
    "    prompt = VERIFIER_PROMPT_TEMPLATE.format(question=q, weak_solution=weak_sol)\n",
    "    PROMPTS_STRONG_VERIFY.append(prompt)\n",
    "\n",
    "print(\"Using stronger model to verify solutions...\")\n",
    "outputs_strong = llm_strong.generate(PROMPTS_STRONG_VERIFY, PARAMS)\n",
    "RESP_STRONG_VERIFIED = [output.outputs[0].text for output in outputs_strong]\n",
    "\n",
    "# Evaluate strong model verification\n",
    "is_correct_strong = []\n",
    "for idx, response in enumerate(RESP_STRONG_VERIFIED):\n",
    "    try:\n",
    "        ans = parse(response)\n",
    "    except Exception:\n",
    "        ans = response\n",
    "    try:\n",
    "        if verify(parse(f\"${GTS_HARD[idx]}$\"), ans):\n",
    "            is_correct_strong.append(1)\n",
    "        else:\n",
    "            is_correct_strong.append(0)\n",
    "    except Exception:\n",
    "        is_correct_strong.append(0)\n",
    "\n",
    "accuracy_strong = sum(is_correct_strong) / len(is_correct_strong) * 100\n",
    "print(f\"Strong Model Verification Accuracy: {accuracy_strong:.2f}% ({sum(is_correct_strong)}/{len(is_correct_strong)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e629b94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Agent-Based Model Routing Framework\n",
    "\n",
    "Use a router agent to decide which model/strategy to use based on question difficulty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0614959",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Literal, Optional, Callable\n",
    "from enum import Enum\n",
    "\n",
    "class DifficultyLevel(Enum):\n",
    "    EASY = \"easy\"\n",
    "    MEDIUM = \"medium\"\n",
    "    HARD = \"hard\"\n",
    "    VERY_HARD = \"very_hard\"\n",
    "\n",
    "class SolverStrategy(Enum):\n",
    "    DIRECT = \"direct\"\n",
    "    COT = \"chain_of_thought\"\n",
    "    MULTI_SAMPLE = \"multi_sample\"\n",
    "    SELF_VERIFY = \"self_verify\"\n",
    "    STRONG_MODEL = \"strong_model\"\n",
    "\n",
    "@dataclass\n",
    "class SolverConfig:\n",
    "    \"\"\"Configuration for a solver strategy\"\"\"\n",
    "    strategy: SolverStrategy\n",
    "    model_name: str\n",
    "    temperature: float\n",
    "    num_samples: int = 1\n",
    "    use_verification: bool = False\n",
    "    \n",
    "@dataclass\n",
    "class Question:\n",
    "    \"\"\"Question with metadata\"\"\"\n",
    "    text: str\n",
    "    ground_truth: str\n",
    "    difficulty_score: float\n",
    "    index: int\n",
    "    \n",
    "@dataclass\n",
    "class SolverResult:\n",
    "    \"\"\"Result from solving a question\"\"\"\n",
    "    question_idx: int\n",
    "    strategy_used: SolverStrategy\n",
    "    response: str\n",
    "    is_correct: int\n",
    "    difficulty_level: DifficultyLevel\n",
    "    num_attempts: int = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a633acf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DifficultyRouter:\n",
    "    \"\"\"Routes questions to appropriate solving strategies based on difficulty.\n",
    "    \n",
    "    Note: difficulty_score is a SUCCESS RATE prediction (0-1):\n",
    "    - Higher values (e.g., 0.8) = EASIER questions (high predicted success)\n",
    "    - Lower values (e.g., 0.2) = HARDER questions (low predicted success)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, very_hard_threshold=0.15, hard_threshold=0.4, medium_threshold=0.7):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            very_hard_threshold: Below this success rate = very hard (use strong model)\n",
    "            hard_threshold: Below this success rate = hard (use multi-sample)\n",
    "            medium_threshold: Below this success rate = medium (use CoT)\n",
    "            Above medium_threshold = easy (direct solve)\n",
    "        \"\"\"\n",
    "        self.very_hard_threshold = very_hard_threshold\n",
    "        self.hard_threshold = hard_threshold\n",
    "        self.medium_threshold = medium_threshold\n",
    "        \n",
    "    def classify_difficulty(self, difficulty_score: float) -> DifficultyLevel:\n",
    "        \"\"\"Classify question difficulty based on predicted success rate.\n",
    "        \n",
    "        Lower success rate = harder question\n",
    "        \"\"\"\n",
    "        if difficulty_score < self.very_hard_threshold:\n",
    "            return DifficultyLevel.VERY_HARD\n",
    "        elif difficulty_score < self.hard_threshold:\n",
    "            return DifficultyLevel.HARD\n",
    "        elif difficulty_score < self.medium_threshold:\n",
    "            return DifficultyLevel.MEDIUM\n",
    "        else:\n",
    "            return DifficultyLevel.EASY\n",
    "    \n",
    "    def route(self, question: Question) -> SolverConfig:\n",
    "        \"\"\"Route question to appropriate solver based on difficulty\"\"\"\n",
    "        difficulty = self.classify_difficulty(question.difficulty_score)\n",
    "        \n",
    "        if difficulty == DifficultyLevel.EASY:\n",
    "            # Easy questions (high success rate): direct solve with weak model\n",
    "            return SolverConfig(\n",
    "                strategy=SolverStrategy.DIRECT,\n",
    "                model_name=\"weak\",\n",
    "                temperature=0.0,\n",
    "                num_samples=1,\n",
    "                use_verification=False\n",
    "            )\n",
    "        elif difficulty == DifficultyLevel.MEDIUM:\n",
    "            # Medium questions (moderate success rate): CoT with weak model\n",
    "            return SolverConfig(\n",
    "                strategy=SolverStrategy.COT,\n",
    "                model_name=\"weak\",\n",
    "                temperature=0.0,\n",
    "                num_samples=1,\n",
    "                use_verification=False\n",
    "            )\n",
    "        elif difficulty == DifficultyLevel.HARD:\n",
    "            # Hard questions (low success rate): Multi-sample with verification\n",
    "            return SolverConfig(\n",
    "                strategy=SolverStrategy.MULTI_SAMPLE,\n",
    "                model_name=\"weak\",\n",
    "                temperature=0.7,\n",
    "                num_samples=5,\n",
    "                use_verification=True\n",
    "            )\n",
    "        else:  # VERY_HARD\n",
    "            # Very hard (very low success rate): Use strong model with verification\n",
    "            return SolverConfig(\n",
    "                strategy=SolverStrategy.STRONG_MODEL,\n",
    "                model_name=\"strong\",\n",
    "                temperature=0.0,\n",
    "                num_samples=1,\n",
    "                use_verification=True\n",
    "            )\n",
    "\n",
    "# Initialize router with corrected thresholds (lower = harder)\n",
    "router = DifficultyRouter(very_hard_threshold=0.15, hard_threshold=0.4, medium_threshold=0.7)\n",
    "print(\"Difficulty Router initialized with SUCCESS RATE thresholds:\")\n",
    "print(f\"  Very Hard: success rate < {0.15}\")\n",
    "print(f\"  Hard: success rate < {0.4}\")\n",
    "print(f\"  Medium: success rate < {0.7}\")\n",
    "print(f\"  Easy: success rate >= {0.7}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34676c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SolverAgent:\n",
    "    \"\"\"Agent that executes different solving strategies\"\"\"\n",
    "    \n",
    "    def __init__(self, weak_model: LLM, strong_model: Optional[LLM] = None):\n",
    "        self.weak_model = weak_model\n",
    "        self.strong_model = strong_model\n",
    "        self.cot_template = \"\"\"{question}\n",
    "\n",
    "Let's approach this step-by-step:\n",
    "1) First, identify what the problem is asking\n",
    "2) Work through the solution carefully\n",
    "3) Check your work by verifying each step\n",
    "4) Provide your final answer\n",
    "\n",
    "Please put your final answer inside \\\\boxed{{}}.\"\"\"\n",
    "\n",
    "        self.verify_template = \"\"\"Original Question: {question}\n",
    "\n",
    "Proposed Solution: {solution}\n",
    "\n",
    "Now, please act as a mathematical reviewer. Carefully verify the solution above:\n",
    "1) Check if all steps are logically sound\n",
    "2) Verify all calculations are correct\n",
    "3) Confirm the final answer makes sense given the problem\n",
    "4) If you find any errors, provide the corrected solution\n",
    "\n",
    "Provide your final verified answer inside \\\\boxed{{}}.\"\"\"\n",
    "\n",
    "    def solve_direct(self, question: Question, config: SolverConfig) -> str:\n",
    "        \"\"\"Direct solve without special prompting\"\"\"\n",
    "        prompt = question.text + \" Please put your final answer inside \\\\boxed{}.\"\n",
    "        params = SamplingParams(temperature=config.temperature, max_tokens=3000)\n",
    "        model = self.strong_model if config.model_name == \"strong\" else self.weak_model\n",
    "        output = model.generate([prompt], params)[0]\n",
    "        return output.outputs[0].text\n",
    "    \n",
    "    def solve_cot(self, question: Question, config: SolverConfig) -> str:\n",
    "        \"\"\"Solve with chain-of-thought prompting\"\"\"\n",
    "        prompt = self.cot_template.format(question=question.text)\n",
    "        params = SamplingParams(temperature=config.temperature, max_tokens=3000)\n",
    "        model = self.strong_model if config.model_name == \"strong\" else self.weak_model\n",
    "        output = model.generate([prompt], params)[0]\n",
    "        return output.outputs[0].text\n",
    "    \n",
    "    def solve_multi_sample(self, question: Question, config: SolverConfig) -> str:\n",
    "        \"\"\"Solve with multiple samples and majority vote\"\"\"\n",
    "        prompt = question.text + \" Please put your final answer inside \\\\boxed{}.\"\n",
    "        params = SamplingParams(temperature=config.temperature, max_tokens=3000, top_p=0.95)\n",
    "        model = self.strong_model if config.model_name == \"strong\" else self.weak_model\n",
    "        \n",
    "        # Generate multiple samples\n",
    "        prompts = [prompt] * config.num_samples\n",
    "        outputs = model.generate(prompts, params)\n",
    "        \n",
    "        # Parse and find majority\n",
    "        from collections import Counter\n",
    "        parsed_answers = []\n",
    "        for output in outputs:\n",
    "            try:\n",
    "                ans = parse(output.outputs[0].text)\n",
    "            except:\n",
    "                ans = output.outputs[0].text\n",
    "            parsed_answers.append(str(ans))\n",
    "        \n",
    "        # Return majority answer (or first if no clear majority)\n",
    "        if parsed_answers:\n",
    "            majority = Counter(parsed_answers).most_common(1)[0][0]\n",
    "            # Return the full response of the first occurrence of majority answer\n",
    "            for i, ans in enumerate(parsed_answers):\n",
    "                if ans == majority:\n",
    "                    return outputs[i].outputs[0].text\n",
    "        return outputs[0].outputs[0].text\n",
    "    \n",
    "    def solve_with_verification(self, question: Question, config: SolverConfig, initial_response: str) -> str:\n",
    "        \"\"\"Verify and potentially correct an initial solution\"\"\"\n",
    "        prompt = self.verify_template.format(question=question.text, solution=initial_response)\n",
    "        params = SamplingParams(temperature=0.0, max_tokens=3000)\n",
    "        model = self.strong_model if config.model_name == \"strong\" else self.weak_model\n",
    "        output = model.generate([prompt], params)[0]\n",
    "        return output.outputs[0].text\n",
    "    \n",
    "    def solve(self, question: Question, config: SolverConfig) -> str:\n",
    "        \"\"\"Main solve method that routes to appropriate strategy\"\"\"\n",
    "        # Get initial solution\n",
    "        if config.strategy == SolverStrategy.DIRECT:\n",
    "            response = self.solve_direct(question, config)\n",
    "        elif config.strategy == SolverStrategy.COT:\n",
    "            response = self.solve_cot(question, config)\n",
    "        elif config.strategy == SolverStrategy.MULTI_SAMPLE:\n",
    "            response = self.solve_multi_sample(question, config)\n",
    "        elif config.strategy == SolverStrategy.STRONG_MODEL:\n",
    "            response = self.solve_direct(question, config)\n",
    "        else:\n",
    "            response = self.solve_direct(question, config)\n",
    "        \n",
    "        # Apply verification if configured\n",
    "        if config.use_verification:\n",
    "            response = self.solve_with_verification(question, config, response)\n",
    "        \n",
    "        return response\n",
    "\n",
    "# Initialize agent with models\n",
    "agent = SolverAgent(weak_model=llm, strong_model=llm_strong if 'llm_strong' in locals() else None)\n",
    "print(\"Solver Agent initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7358e4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgenticSolver:\n",
    "    \"\"\"Main orchestrator that coordinates routing and solving\"\"\"\n",
    "    \n",
    "    def __init__(self, router: DifficultyRouter, agent: SolverAgent):\n",
    "        self.router = router\n",
    "        self.agent = agent\n",
    "        self.results = []\n",
    "        \n",
    "    def solve_question(self, question: Question) -> SolverResult:\n",
    "        \"\"\"Solve a single question using agent-based routing\"\"\"\n",
    "        # Route to appropriate strategy\n",
    "        config = self.router.route(question)\n",
    "        difficulty = self.router.classify_difficulty(question.difficulty_score)\n",
    "        \n",
    "        # Solve using selected strategy\n",
    "        response = self.agent.solve(question, config)\n",
    "        \n",
    "        # Verify answer\n",
    "        is_correct = verify_answer(question.ground_truth, response)\n",
    "        \n",
    "        # Create result\n",
    "        result = SolverResult(\n",
    "            question_idx=question.index,\n",
    "            strategy_used=config.strategy,\n",
    "            response=response,\n",
    "            is_correct=is_correct,\n",
    "            difficulty_level=difficulty,\n",
    "            num_attempts=config.num_samples\n",
    "        )\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def solve_batch(self, questions: list[Question], verbose: bool = True) -> list[SolverResult]:\n",
    "        \"\"\"Solve a batch of questions\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for question in tqdm(questions, desc=\"Solving with agent\", disable=not verbose):\n",
    "            result = self.solve_question(question)\n",
    "            results.append(result)\n",
    "        \n",
    "        self.results = results\n",
    "        return results\n",
    "    \n",
    "    def get_summary(self) -> pd.DataFrame:\n",
    "        \"\"\"Get summary statistics of solving results\"\"\"\n",
    "        if not self.results:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        summary_data = []\n",
    "        for difficulty_level in DifficultyLevel:\n",
    "            level_results = [r for r in self.results if r.difficulty_level == difficulty_level]\n",
    "            if level_results:\n",
    "                accuracy = sum(r.is_correct for r in level_results) / len(level_results) * 100\n",
    "                strategy_counts = {}\n",
    "                for r in level_results:\n",
    "                    strategy_counts[r.strategy_used.value] = strategy_counts.get(r.strategy_used.value, 0) + 1\n",
    "                \n",
    "                summary_data.append({\n",
    "                    'Difficulty': difficulty_level.value,\n",
    "                    'Count': len(level_results),\n",
    "                    'Accuracy': accuracy,\n",
    "                    'Correct': sum(r.is_correct for r in level_results),\n",
    "                    'Primary_Strategy': max(strategy_counts, key=strategy_counts.get),\n",
    "                })\n",
    "        \n",
    "        return pd.DataFrame(summary_data)\n",
    "\n",
    "# Initialize the agentic solver\n",
    "agentic_solver = AgenticSolver(router=router, agent=agent)\n",
    "print(\"Agentic Solver initialized and ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80b6762",
   "metadata": {},
   "source": [
    "### Run Agentic Solver on Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3eb4cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare questions from dataframe\n",
    "questions = []\n",
    "for idx, row in df.iterrows():\n",
    "    question = Question(\n",
    "        text=row['question'],\n",
    "        ground_truth=row['answer'],\n",
    "        difficulty_score=row['predicted_difficulty_sigmoid'],\n",
    "        index=idx\n",
    "    )\n",
    "    questions.append(question)\n",
    "\n",
    "print(f\"Prepared {len(questions)} questions for agentic solving\")\n",
    "\n",
    "# Show routing distribution\n",
    "difficulty_counts = {}\n",
    "for q in questions:\n",
    "    config = router.route(q)\n",
    "    difficulty = router.classify_difficulty(q.difficulty_score)\n",
    "    key = f\"{difficulty.value} -> {config.strategy.value}\"\n",
    "    difficulty_counts[key] = difficulty_counts.get(key, 0) + 1\n",
    "\n",
    "print(\"\\nRouting distribution:\")\n",
    "for key, count in sorted(difficulty_counts.items()):\n",
    "    print(f\"  {key}: {count} questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e72059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve questions using agentic approach\n",
    "print(\"Starting agentic solving...\")\n",
    "agentic_results = agentic_solver.solve_batch(questions, verbose=True)\n",
    "print(f\"\\nCompleted solving {len(agentic_results)} questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c5d82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get summary by difficulty level\n",
    "summary_df = agentic_solver.get_summary()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"AGENTIC SOLVER RESULTS BY DIFFICULTY\")\n",
    "print(\"=\"*70)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Overall accuracy\n",
    "total_correct = sum(r.is_correct for r in agentic_results)\n",
    "total_questions = len(agentic_results)\n",
    "overall_accuracy = total_correct / total_questions * 100\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}% ({total_correct}/{total_questions})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c7d742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed results dataframe\n",
    "agentic_results_df = pd.DataFrame([\n",
    "    {\n",
    "        'question_idx': r.question_idx,\n",
    "        'difficulty_score': questions[r.question_idx].difficulty_score,\n",
    "        'difficulty_level': r.difficulty_level.value,\n",
    "        'strategy': r.strategy_used.value,\n",
    "        'is_correct': r.is_correct,\n",
    "        'num_attempts': r.num_attempts\n",
    "    }\n",
    "    for r in agentic_results\n",
    "])\n",
    "\n",
    "agentic_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003044ae",
   "metadata": {},
   "source": [
    "### Visualize Agent Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f12066e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Accuracy by difficulty level\n",
    "summary_df_sorted = summary_df.sort_values('Difficulty')\n",
    "axes[0].bar(summary_df_sorted['Difficulty'], summary_df_sorted['Accuracy'])\n",
    "axes[0].set_xlabel('Difficulty Level')\n",
    "axes[0].set_ylabel('Accuracy (%)')\n",
    "axes[0].set_title('Agentic Solver Accuracy by Difficulty')\n",
    "axes[0].set_ylim([0, 100])\n",
    "for i, row in summary_df_sorted.iterrows():\n",
    "    axes[0].text(i, row['Accuracy'] + 2, f\"{row['Accuracy']:.1f}%\", ha='center')\n",
    "\n",
    "# Plot 2: Strategy distribution\n",
    "strategy_counts = agentic_results_df['strategy'].value_counts()\n",
    "axes[1].bar(strategy_counts.index, strategy_counts.values)\n",
    "axes[1].set_xlabel('Strategy')\n",
    "axes[1].set_ylabel('Number of Questions')\n",
    "axes[1].set_title('Strategy Usage Distribution')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "for i, (strategy, count) in enumerate(strategy_counts.items()):\n",
    "    axes[1].text(i, count + 0.5, str(count), ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Agent performance visualized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94710b5d",
   "metadata": {},
   "source": [
    "### Compare Agentic vs Fixed Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de91c901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare agentic solver to baseline (direct solving)\n",
    "print(\"Comparison of Agentic vs Baseline Approach\\n\")\n",
    "\n",
    "# Baseline: original direct solve (from earlier in notebook)\n",
    "if 'is_correct' in locals():\n",
    "    baseline_accuracy = sum(is_correct) / len(is_correct) * 100\n",
    "    baseline_correct = sum(is_correct)\n",
    "    baseline_total = len(is_correct)\n",
    "else:\n",
    "    baseline_accuracy = 0\n",
    "    baseline_correct = 0\n",
    "    baseline_total = 0\n",
    "\n",
    "# Agentic\n",
    "agentic_accuracy = overall_accuracy\n",
    "agentic_correct = total_correct\n",
    "agentic_total = total_questions\n",
    "\n",
    "comparison_data = {\n",
    "    'Approach': ['Baseline (Direct)', 'Agentic (Adaptive)'],\n",
    "    'Accuracy (%)': [baseline_accuracy, agentic_accuracy],\n",
    "    'Correct': [baseline_correct, agentic_correct],\n",
    "    'Total': [baseline_total, agentic_total],\n",
    "    'Improvement': [0, agentic_accuracy - baseline_accuracy]\n",
    "}\n",
    "\n",
    "comparison_table = pd.DataFrame(comparison_data)\n",
    "print(comparison_table.to_string(index=False))\n",
    "\n",
    "print(f\"\\n✓ Agentic approach improves accuracy by {agentic_accuracy - baseline_accuracy:.2f} percentage points\")\n",
    "print(f\"✓ Routes questions intelligently based on difficulty\")\n",
    "print(f\"✓ Uses stronger strategies only when needed (cost-efficient)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dfr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
