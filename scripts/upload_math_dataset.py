from huggingface_hub import create_repo
from datasets import Dataset, DatasetDict
import pandas as pd
import os

# Dataset README content
DATASET_README = """---
dataset_info:
  configs:
    - config_name: openai--gpt-oss-20b_low
      data_files:
        - split: train
          path: openai--gpt-oss-20b_low/train
        - split: validation
          path: openai--gpt-oss-20b_low/validation
        - split: test
          path: openai--gpt-oss-20b_low/test
      features:
        - name: problem
          dtype: string
        - name: generation
          dtype: string
        - name: majority_vote_is_correct
          dtype: int8
        - name: rating
          dtype: float64
        - name: k
          dtype: int64
        - name: temperature
          dtype: float64
        - name: max_len
          dtype: int64
    - config_name: openai--gpt-oss-20b_high
      data_files:
        - split: train
          path: openai--gpt-oss-20b_high/train
        - split: validation
          path: openai--gpt-oss-20b_high/validation
        - split: test
          path: openai--gpt-oss-20b_high/test
      features:
        - name: problem
          dtype: string
        - name: generation
          dtype: string
        - name: majority_vote_is_correct
          dtype: int8
        - name: rating
          dtype: float64
        - name: k
          dtype: int64
        - name: temperature
          dtype: float64
        - name: max_len
          dtype: int64
language:
  - en
license: mit
multilinguality:
  - monolingual
size_categories:
  - unknown
source_datasets:
  - original
tags:
  - code-generation
  - math
  - mathematics
  - evaluation
pretty_name: PIKA MATH Generations
---

# PIKA MATH Generations Dataset

A comprehensive dataset of MATH problem solutions generated by different language models with various sampling parameters.

## Dataset Description

This dataset contains code generation results from the [MATH Dataset](https://github.com/hendrycks/math) evaluated across multiple models. Each entry includes the generated solution, correctness scores, and generation hyperparameters.

### Generation Parameters Used

- **k (num_samples)**: {k}
- **temperature**: {temperature}
- **max_len**: {max_len}

## Dataset Structure

### Columns

| Column | Type | Description |
|--------|------|-------------|
| `problem` | str | The MATH problem statement |
| `generation` | str | The generated solution |
| `majority_vote_is_correct` | int (0/1) | Whether the majority vote is correct |
| `rating` | float | Quality/correctness rating |
| `k` | int | Number of samples generated |
| `temperature` | float | Sampling temperature |
| `max_len` | int | Maximum generation length |

### Data Splits

- **train**: Training split
- **validation**: Validation split
- **test**: Test split

## Configurations

The dataset includes the following configurations:

- `openai--gpt-oss-20b_low`: GPT-OSS-20B model with low difficulty problems
- `openai--gpt-oss-20b_high`: GPT-OSS-20B model with high difficulty problems

## Usage

```python
from datasets import load_dataset

# Load a specific model configuration
dataset = load_dataset(
    "CoffeeGitta/pika-math-generations",
    name="openai--gpt-oss-20b_low"
)

# Get training split
train = dataset["train"]

# Filter by correctness
correct = train.filter(lambda x: x["majority_vote_is_correct"] == 1)
```

## Citation

```bibtex
@article{lugoloobi_llms_2026,
    title = {{LLMs} {Encode} {Their} {Failures}: {Predicting} {Success} from {Pre}-{Generation} {Activations}},
    shorttitle = {{LLMs} {Encode} {Their} {Failures}},
    url = {http://arxiv.org/abs/2602.09924},
    doi = {10.48550/arXiv.2602.09924},
    abstract = {Running LLMs with extended reasoning on every problem is expensive, but determining which inputs actually require additional compute remains challenging. We investigate whether their own likelihood of success is recoverable from their internal representations before generation, and if this signal can guide more efficient inference. We train linear probes on pre-generation activations to predict policy-specific success on math and coding tasks, substantially outperforming surface features such as question length and TF-IDF. Using E2H-AMC, which provides both human and model performance on identical problems, we show that models encode a model-specific notion of difficulty that is distinct from human difficulty, and that this distinction increases with extended reasoning. Leveraging these probes, we demonstrate that routing queries across a pool of models can exceed the best-performing model whilst reducing inference cost by up to 70\% on MATH, showing that internal representations enable practical efficiency gains even when they diverge from human intuitions about difficulty.},
    publisher = {arXiv},
    author = {Lugoloobi, William and Foster, Thomas and Bankes, William and Russell, Chris},
    month = feb,
    year = {2026},
    note = {arXiv:2602.09924 [cs]},
    keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{hendrycks_measuring_2021,
	title = {Measuring {Mathematical} {Problem} {Solving} {With} the {MATH} {Dataset}},
	journal = {arXiv preprint arXiv:2103.03874},
	author = {Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},
	year = {2021},
}
```
"""


def enforce_generation_columns(df, k, temperature, max_len):
    """
    Force generation args to exist and be constant.
    """
    df["k"] = k
    df["temperature"] = temperature
    df["max_len"] = max_len
    return df


def enforce_correctness_column(df):
    """
    Ensure majority_vote_is_correct is int8 (0 or 1).
    """
    if "majority_vote_is_correct" in df.columns:
        df["majority_vote_is_correct"] = (
            df["majority_vote_is_correct"]
            .fillna(0)
            .astype("int8")
        )

        # Optional strict check
        unique_vals = set(df["majority_vote_is_correct"].unique())
        if not unique_vals.issubset({0, 1}):
            raise ValueError(
                f"majority_vote_is_correct contains invalid values: {unique_vals}"
            )

    return df


def normalize_schemas(split_dfs):
    """
    Ensure all splits have identical columns.
    """
    all_columns = set()
    for df in split_dfs.values():
        all_columns.update(df.columns)

    for split, df in split_dfs.items():
        missing = all_columns - set(df.columns)
        for col in missing:
            df[col] = pd.NA
        split_dfs[split] = df[sorted(all_columns)]

    return split_dfs


def push_math_dataset(
    base_dir,
    repo_id,
    dataset_name,
    models,
    k,
    temperature,
    max_len,
    push_readme=True,
):
    """
    models = list of (org, model)
    """
    from huggingface_hub import HfApi

    create_repo(repo_id, repo_type="dataset", exist_ok=True)
    
    # Push README if requested
    if push_readme:
        api = HfApi()
        # Use safe string replacement instead of .format() to handle YAML braces
        readme_content = DATASET_README.replace('{k}', str(k)).replace(
            '{temperature}', str(temperature)
        ).replace('{max_len}', str(max_len))
        
        api.upload_file(
            path_or_fileobj=readme_content.encode('utf-8'),
            path_in_repo="README.md",
            repo_id=repo_id,
            repo_type="dataset",
        )
        print(f"✅ Uploaded README to {repo_id}")

    for org, model in models:

        print(f"\nProcessing {org}/{model}")

        config_name = f"{org}--{model}"
        split_dfs = {}

        for split in ["train", "val", "test"]:


            file_path = os.path.join(
                base_dir,
                org,
                model,
                dataset_name,
                f"{split}_maxlen_{max_len}_k_{k}_temp_{temperature}.parquet"
            )

            if not os.path.exists(file_path):
                print(f"{split} not found for {config_name}")
                continue

            df = pd.read_parquet(file_path)

            # Enforce numeric consistency
            if "rating" in df.columns:
                df["rating"] = df["rating"].astype("float64")
            else:
                df["rating"] = pd.Series([None] * len(df), dtype="float64")


            # Standardize schema
            if "problem" in df.columns:
                pass  # Keep column as "problem" to match YAML metadata
            
            # Rename "question" to "problem" if needed
            if "question" in df.columns and "problem" not in df.columns:
                df = df.rename(columns={"question": "problem"})

            if "idx" in df.columns:
                df = df.drop(columns=["idx"])

            # Enforce generation config columns
            df = enforce_generation_columns(df, k, temperature, max_len)

            # Enforce correctness dtype
            df = enforce_correctness_column(df)

            split_name = "validation" if split == "val" else split

            print(f"{split_name}: {len(df)} rows")

            split_dfs[split_name] = df

        if not split_dfs:
            print(f"No splits found for {config_name}")
            continue

        # Normalize schemas across splits
        split_dfs = normalize_schemas(split_dfs)

        # Convert to HF DatasetDict
        dataset_dict = DatasetDict({
            split: Dataset.from_pandas(df, preserve_index=False)
            for split, df in split_dfs.items()
        })

        # Push to hub
        dataset_dict.push_to_hub(
            repo_id,
            config_name=config_name,
            private=False,
        )

        print(f"✅ Uploaded {config_name}")


if __name__ == "__main__":

    BASE_DIR = "data"
    REPO_ID = "CoffeeGitta/pika-math-generations"
    DATASET_NAME = "DigitalLearningGmbH_MATH-lighteval"

    MODELS = [
        ("openai", "gpt-oss-20b_low"),
        ("openai", "gpt-oss-20b_high"),
        ("openai", "gpt-oss-20b_medium"),
    ]

    push_math_dataset(
        base_dir=BASE_DIR,
        repo_id=REPO_ID,
        dataset_name=DATASET_NAME,
        models=MODELS,
        k=5,
        temperature=1.0,
        max_len=131072,
        push_readme=True,  # Set to False if you've already pushed README
    )
