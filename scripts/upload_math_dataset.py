from huggingface_hub import create_repo
from datasets import Dataset, DatasetDict
import pandas as pd
import os

# Dataset README content
DATASET_README = r"""---
dataset_info:
  configs:
    - config_name: openai--gpt-oss-20b_low
      data_files:
        - split: train
          path: openai--gpt-oss-20b_low/train
        - split: validation
          path: openai--gpt-oss-20b_low/validation
        - split: test
          path: openai--gpt-oss-20b_low/test
      features:
        - name: problem
          dtype: string
        - name: generation
          dtype: string
        - name: majority_vote_is_correct
          dtype: int8
        - name: rating
          dtype: float64
        - name: k
          dtype: int64
        - name: temperature
          dtype: float64
        - name: max_len
          dtype: int64
    - config_name: openai--gpt-oss-20b_high
      data_files:
        - split: train
          path: openai--gpt-oss-20b_high/train
        - split: validation
          path: openai--gpt-oss-20b_high/validation
        - split: test
          path: openai--gpt-oss-20b_high/test
      features:
        - name: problem
          dtype: string
        - name: generation
          dtype: string
        - name: majority_vote_is_correct
          dtype: int8
        - name: rating
          dtype: float64
        - name: k
          dtype: int64
        - name: temperature
          dtype: float64
        - name: max_len
          dtype: int64
language:
  - en
license: mit
multilinguality:
  - monolingual
size_categories:
  - unknown
source_datasets:
  - original
tags:
  - code-generation
  - math
  - mathematics
  - evaluation
pretty_name: PIKA MATH Generations
---

# PIKA MATH Generations Dataset

A comprehensive dataset of MATH problem solutions generated by different language models with various sampling parameters.

## Dataset Description

This dataset contains code generation results from the [MATH Dataset](https://github.com/hendrycks/math) evaluated across multiple models. Each entry includes the generated solution, correctness scores, and generation hyperparameters.

### Generation Parameters Used

- **k (num_samples)**: {k}
- **temperature**: {temperature}
- **max_len**: {max_len}

## Dataset Structure

### Columns

| Column | Type | Description |
|--------|------|-------------|
| `problem` | str | The MATH problem statement |
| `generation` | str | The generated solution |
| `majority_vote_is_correct` | int (0/1) | Whether the majority vote is correct |
| `rating` | float | Quality/correctness rating |
| `k` | int | Number of samples generated |
| `temperature` | float | Sampling temperature |
| `max_len` | int | Maximum generation length |

### Data Splits

- **train**: Training split
- **validation**: Validation split
- **test**: Test split

## Configurations

The dataset includes the following configurations:

- `openai--gpt-oss-20b_low`: GPT-OSS-20B model with low difficulty problems
- `openai--gpt-oss-20b_high`: GPT-OSS-20B model with high difficulty problems

## Usage

```python
from datasets import load_dataset

# Load a specific model configuration
dataset = load_dataset(
    "CoffeeGitta/pika-math-generations",
    name="openai--gpt-oss-20b_low"
)

# Get training split
train = dataset["train"]

# Filter by correctness
correct = train.filter(lambda x: x["majority_vote_is_correct"] == 1)
```

## Citation

```bibtex
@article{lugoloobi_llms_2026,
    title = {{LLMs} {Encode} {Their} {Failures}: {Predicting} {Success} from {Pre}-{Generation} {Activations}},
    shorttitle = {{LLMs} {Encode} {Their} {Failures}},
    url = {http://arxiv.org/abs/2602.09924},
    doi = {10.48550/arXiv.2602.09924},
    abstract = {Running LLMs with extended reasoning on every problem is expensive, but determining which inputs actually require additional compute remains challenging. We investigate whether their own likelihood of success is recoverable from their internal representations before generation, and if this signal can guide more efficient inference. We train linear probes on pre-generation activations to predict policy-specific success on math and coding tasks, substantially outperforming surface features such as question length and TF-IDF. Using E2H-AMC, which provides both human and model performance on identical problems, we show that models encode a model-specific notion of difficulty that is distinct from human difficulty, and that this distinction increases with extended reasoning. Leveraging these probes, we demonstrate that routing queries across a pool of models can exceed the best-performing model whilst reducing inference cost by up to 70\% on MATH, showing that internal representations enable practical efficiency gains even when they diverge from human intuitions about difficulty.},
    publisher = {arXiv},
    author = {Lugoloobi, William and Foster, Thomas and Bankes, William and Russell, Chris},
    month = feb,
    year = {2026},
    note = {arXiv:2602.09924 [cs]},
    keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{hendrycks_measuring_2021,
	title = {Measuring {Mathematical} {Problem} {Solving} {With} the {MATH} {Dataset}},
	journal = {arXiv preprint arXiv:2103.03874},
	author = {Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},
	year = {2021},
}
```
"""


def enforce_generation_columns(df, k, temperature, max_len):
    """
    Force generation args to exist and be constant.
    """
    df["k"] = k
    df["temperature"] = temperature
    df["max_len"] = max_len
    return df


def enforce_correctness_column(df):
    """
    Ensure majority_vote_is_correct is int8 (0 or 1).
    """
    if "majority_vote_is_correct" in df.columns:
        df["majority_vote_is_correct"] = (
            df["majority_vote_is_correct"]
            .fillna(0)
            .astype("int8")
        )

        # Optional strict check
        unique_vals = set(df["majority_vote_is_correct"].unique())
        if not unique_vals.issubset({0, 1}):
            raise ValueError(
                f"majority_vote_is_correct contains invalid values: {unique_vals}"
            )

    return df


def normalize_schemas(split_dfs):
    """
    Ensure all splits have identical columns.
    """
    all_columns = set()
    for df in split_dfs.values():
        all_columns.update(df.columns)

    for split, df in split_dfs.items():
        missing = all_columns - set(df.columns)
        for col in missing:
            df[col] = pd.NA
        split_dfs[split] = df[sorted(all_columns)]

    return split_dfs


def find_data_file(model_path, split, k=5):
    """
    Auto-detect available parquet file for a given split with specific k.
    Looks for files matching: {split}_maxlen_*_k_{k}_temp_*.parquet
    """
    if not os.path.exists(model_path):
        return None
    
    import glob
    pattern = os.path.join(model_path, f"{split}_maxlen_*_k_{k}_temp_*.parquet")
    files = glob.glob(pattern)
    
    if files:
        # Return the first match (or most recent if multiple)
        return sorted(files)[-1]
    return None


def extract_params_from_path(file_path):
    """
    Extract k, temperature, max_len from filename like:
    train_maxlen_3000_k_5_temp_0.7.parquet
    """
    import re
    filename = os.path.basename(file_path)
    
    # Remove .parquet extension first
    filename = filename.replace('.parquet', '')
    
    # Extract parameters from filename
    maxlen_match = re.search(r'maxlen_(\d+)', filename)
    k_match = re.search(r'k_(\d+)', filename)
    temp_match = re.search(r'temp_([\d.]+)', filename)
    
    max_len = int(maxlen_match.group(1)) if maxlen_match else 3000
    k = int(k_match.group(1)) if k_match else 5
    temperature = float(temp_match.group(1)) if temp_match else 0.7
    
    return k, temperature, max_len


def push_math_dataset(
    base_dir,
    repo_id,
    dataset_name,
    models,
    push_readme=True,
):
    """
    models = list of (org, model, k, temperature, max_len) or (org, model)
    If hyperparams not provided, will auto-detect from available files.
    """
    from huggingface_hub import HfApi

    create_repo(repo_id, repo_type="dataset", exist_ok=True)
    
    # Push README if requested
    if push_readme:
        api = HfApi()
        # Use safe string replacement instead of .format() to handle YAML braces
        readme_content = DATASET_README.replace('{k}', '5').replace(
            '{temperature}', '~0.7-1.0'
        ).replace('{max_len}', '~3000-131072')
        
        api.upload_file(
            path_or_fileobj=readme_content.encode('utf-8'),
            path_in_repo="README.md",
            repo_id=repo_id,
            repo_type="dataset",
        )
        print(f"✅ Uploaded README to {repo_id}")

    for model_info in models:
        # Support both (org, model) and (org, model, k, temp, max_len) tuples
        if len(model_info) == 2:
            org, model = model_info
            k, temperature, max_len = 5, None, None  # Default k=5, auto-detect temp and max_len
        elif len(model_info) == 5:
            org, model, k, temperature, max_len = model_info
        else:
            raise ValueError(f"Model info must be (org, model) or (org, model, k, temp, max_len), got {model_info}")

        print(f"\nProcessing {org}/{model}")

        config_name = f"{org}--{model}"
        split_dfs = {}

        for split in ["train", "val", "test"]:

            if model == "gpt-oss-20b_medium" and split in ["train", "val"]:
                continue

            # Build model path
            model_path = os.path.join(base_dir, org, model, dataset_name)
            
            # If hyperparams not specified, auto-detect from available files (with k=5 enforced)
            if temperature is None or max_len is None:
                file_path = find_data_file(model_path, split, k=5)
                if not file_path:
                    print(f"{split} not found for {config_name} (k=5)")
                    continue
                # Extract actual parameters from filename
                actual_k, actual_temp, actual_maxlen = extract_params_from_path(file_path)
            else:
                file_path = os.path.join(
                    model_path,
                    f"{split}_maxlen_{max_len}_k_{k}_temp_{temperature}.parquet"
                )
                actual_k, actual_temp, actual_maxlen = k, temperature, max_len

            if not os.path.exists(file_path):
                print(f"{split} not found for {config_name}")
                continue

            df = pd.read_parquet(file_path)

            # Enforce numeric consistency
            if "rating" in df.columns:
                df["rating"] = df["rating"].astype("float64")
            else:
                df["rating"] = pd.Series([None] * len(df), dtype="float64")


            # Standardize schema
            if "problem" in df.columns:
                pass  # Keep column as "problem" to match YAML metadata
            
            # Rename "question" to "problem" if needed
            if "question" in df.columns and "problem" not in df.columns:
                df = df.rename(columns={"question": "problem"})

            if "idx" in df.columns:
                df = df.drop(columns=["idx"])

            # Enforce generation config columns with actual parameters
            df = enforce_generation_columns(df, actual_k, actual_temp, actual_maxlen)

            # Enforce correctness dtype
            df = enforce_correctness_column(df)

            split_name = "validation" if split == "val" else split

            print(f"{split_name}: {len(df)} rows")

            split_dfs[split_name] = df

        if not split_dfs:
            print(f"No splits found for {config_name}")
            continue

        # Normalize schemas across splits
        split_dfs = normalize_schemas(split_dfs)

        # Convert to HF DatasetDict
        dataset_dict = DatasetDict({
            split: Dataset.from_pandas(df, preserve_index=False)
            for split, df in split_dfs.items()
        })

        # Push to hub
        dataset_dict.push_to_hub(
            repo_id,
            config_name=config_name,
            private=False,
        )

        print(f"✅ Uploaded {config_name}")


if __name__ == "__main__":

    BASE_DIR = "data"
    REPO_ID = "CoffeeGitta/pika-math-generations"
    DATASET_NAME = "DigitalLearningGmbH_MATH-lighteval"

    # Models can be specified as:
    # - (org, model) - will auto-detect parameters from filenames
    # - (org, model, k, temperature, max_len) - uses explicit parameters
    MODELS = [
        ("openai", "gpt-oss-20b_low"),
        ("openai", "gpt-oss-20b_high"),
        ("openai", "gpt-oss-20b_medium"),
        ("Qwen", "Qwen2.5-Math-1.5B-Instruct"),  # Auto-detects from available files
        ("Qwen", "Qwen2.5-Math-7B-Instruct"),    # Auto-detects from available files
        ("deepseek-ai", "DeepSeek-R1-Distill-Qwen-7B"),    # Auto-detects from available files
    ]

    push_math_dataset(
        base_dir=BASE_DIR,
        repo_id=REPO_ID,
        dataset_name=DATASET_NAME,
        models=MODELS,
        push_readme=True,  # Set to False if you've already pushed README
    )
