---
dataset_info:
  configs:
    - config_name: openai--gpt-oss-20b_low
      data_files:
        - split: train
          path: openai--gpt-oss-20b_low/train
        - split: validation
          path: openai--gpt-oss-20b_low/validation
        - split: test
          path: openai--gpt-oss-20b_low/test
      features:
        - name: problem
          dtype: string
        - name: generation
          dtype: string
        - name: majority_vote_is_correct
          dtype: int8
        - name: rating
          dtype: float64
        - name: k
          dtype: int64
        - name: temperature
          dtype: float64
        - name: max_len
          dtype: int64
    - config_name: openai--gpt-oss-20b_high
      data_files:
        - split: train
          path: openai--gpt-oss-20b_high/train
        - split: validation
          path: openai--gpt-oss-20b_high/validation
        - split: test
          path: openai--gpt-oss-20b_high/test
      features:
        - name: problem
          dtype: string
        - name: generation
          dtype: string
        - name: majority_vote_is_correct
          dtype: int8
        - name: rating
          dtype: float64
        - name: k
          dtype: int64
        - name: temperature
          dtype: float64
        - name: max_len
          dtype: int64
language:
  - en
license: mit
multilinguality:
  - monolingual
size_categories:
  - unknown
source_datasets:
  - original
tags:
  - code-generation
  - math
  - mathematics
  - evaluation
pretty_name: PIKA MATH Generations
---

# PIKA MATH Generations Dataset

A comprehensive dataset of MATH problem solutions generated by different language models with various sampling parameters.

## Dataset Description

This dataset contains code generation results from the [MATH Dataset](https://github.com/hendrycks/math) (Hendrycks et al., 2021) evaluated across multiple models. Each entry includes the generated solution, correctness scores, and generation hyperparameters.

### Dataset Details

- **Size**: Varies by model and split (train/validation/test)
- **Language**: Python code
- **Task**: Mathematical problem solving
- **Generation Parameters**: k=5, temperature=1.0, max_len=131072

## Dataset Structure

### Columns

| Column | Type | Description |
|--------|------|-------------|
| `problem` | str | The MATH problem statement (prompt) |
| `generation` | str | The generated solution code |
| `majority_vote_is_correct` | int (0/1) | Whether majority vote across samples is correct |
| `rating` | float | Quality/correctness rating |
| `k` | int | Number of samples generated |
| `temperature` | float | Sampling temperature |
| `max_len` | int | Maximum generation length |

### Data Splits

The dataset is organized by model in separate configurations:
- **train**: Training split
- **validation**: Validation split
- **test**: Test split

Each configuration contains results from a specific model with the exact same generation parameters.

## Configurations

The dataset includes the following model/configuration pairs:

- `openai--gpt-oss-20b_low`: GPT-OSS-20B model with low difficulty problems
- `openai--gpt-oss-20b_high`: GPT-OSS-20B model with high difficulty problems

`"openai--gpt-oss-20b_medium"` currently only has a test set uploaded. Train and val will be added soon.


## Generation Details

All generations were produced with:
- **k (num_samples)**: 5 samples per problem
- **temperature**: 1.0 (stochastic sampling)
- **max_len**: 131072 tokens

These parameters ensure diverse, exploratory generation with sufficient tokens for complex solutions.

## Data Fields

```python
{
  'problem': 'str',           # Original MATH problem
  'generation': 'str',        # Generated solution
  'majority_vote_is_correct': 'int8',  # Correctness (0 or 1)
  'rating': 'float64',        # Quality score
  'k': 'int64',               # Samples per problem
  'temperature': 'float64',   # Sampling temperature
  'max_len': 'int64'          # Max generation length
}
```

## Usage

### Load with Hugging Face `datasets`

```python
from datasets import load_dataset

# Load a specific model configuration
dataset = load_dataset(
    "CoffeeGitta/pika-math-generations",
    name="openai--gpt-oss-20b_low"
)

# Access a split
train_data = dataset["train"]

# Inspect an example
print(train_data[0])
```

### Filter by Correctness

```python
# Get only correct solutions
correct = dataset["train"].filter(
    lambda x: x["majority_vote_is_correct"] == 1
)

# Get only incorrect solutions
incorrect = dataset["train"].filter(
    lambda x: x["majority_vote_is_correct"] == 0
)
```

### Access Different Models

```python
# Load high difficulty problems
high_diff = load_dataset(
    "CoffeeGitta/pika-math-generations",
    name="openai--gpt-oss-20b_high"
)

# Load low difficulty problems
low_diff = load_dataset(
    "CoffeeGitta/pika-math-generations",
    name="openai--gpt-oss-20b_low"
)
```


## Source Data

This dataset is based on the [MATH Dataset](DigitalLearningGmbH/MATH-lighteval) (Hendrycks et al., 2021), which contains 12,500 challenging high school math competition problems.

## Citation
If you find this helpful please cite:
```bibtex
@misc{lugoloobi_llms_2026,
    title = {{LLMs} {Encode} {Their} {Failures}: {Predicting} {Success} from {Pre}-{Generation} {Activations}},
    shorttitle = {{LLMs} {Encode} {Their} {Failures}},
    url = {http://arxiv.org/abs/2602.09924},
    doi = {10.48550/arXiv.2602.09924},
    abstract = {Running LLMs with extended reasoning on every problem is expensive, but determining which inputs actually require additional compute remains challenging. We investigate whether their own likelihood of success is recoverable from their internal representations before generation, and if this signal can guide more efficient inference. We train linear probes on pre-generation activations to predict policy-specific success on math and coding tasks, substantially outperforming surface features such as question length and TF-IDF. Using E2H-AMC, which provides both human and model performance on identical problems, we show that models encode a model-specific notion of difficulty that is distinct from human difficulty, and that this distinction increases with extended reasoning. Leveraging these probes, we demonstrate that routing queries across a pool of models can exceed the best-performing model whilst reducing inference cost by up to 70\% on MATH, showing that internal representations enable practical efficiency gains even when they diverge from human intuitions about difficulty.},
    publisher = {arXiv},
    author = {Lugoloobi, William and Foster, Thomas and Bankes, William and Russell, Chris},
    month = feb,
    year = {2026},
    note = {arXiv:2602.09924 [cs]},
    keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{hendrycks_measuring_2021,
	title = {Measuring {Mathematical} {Problem} {Solving} {With} the {MATH} {Dataset}},
	journal = {arXiv preprint arXiv:2103.03874},
	author = {Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},
	year = {2021},
}
```


## License

This dataset follows the same license as the original MATH dataset. Please refer to the [MATH Dataset](https://github.com/hendrycks/math) for license details.

## Acknowledgments

- MATH Dataset: Hendrycks et al. (2021)

## Questions?

For issues, questions, or contributions, please open an issue on the [project repository](https://github.com/KabakaWilliam/llms_know_difficulty).
