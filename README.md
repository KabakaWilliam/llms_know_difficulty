# LLMs Encode Their Failures: Predicting Success from Pre-Generation Activations

## Setup

Requires **conda** and a CUDA-capable GPU.

```bash
# Clone and enter the repo
git clone https://github.com/KabakaWilliam/llms_know_difficulty.git
cd llms_know_difficulty

# Create the conda env and install pika in editable mode
bash setup.sh

# Activate
conda activate pika
```

### Generate success-rate datasets

Requires `vllm` (`pip install vllm`):

```bash
cd create_sr_datasets
python create_pareto_sr_datasets.py
```

## PIKA â€” Probe-Informed K-Aware Routing

PIKA trains lightweight probes on LLM internal representations to predict per-problem difficulty, then uses those predictions to route queries across a pool of models â€” balancing accuracy against inference cost.

## Repository Structure

```
â”œâ”€â”€ src/pika/                  # Core library (pip-installable)
â”‚   â”œâ”€â”€ main.py                # CLI: train a probe on a dataset
â”‚   â”œâ”€â”€ predict_with_probe.py  # CLI: run inference with a trained probe
â”‚   â”œâ”€â”€ probe/                 # Probe implementations
â”‚   â”‚   â”œâ”€â”€ probe_factory.py   #   Factory for all probe types
â”‚   â”‚   â”œâ”€â”€ linear_eoi_probe.py
â”‚   â”‚   â”œâ”€â”€ tfidf_probe.py
â”‚   â”‚   â”œâ”€â”€ torch_probe.py     #   Attn, LinearThenMax, etc.
â”‚   â”‚   â””â”€â”€ length_probe.py    #   Token-length baseline
â”‚   â”œâ”€â”€ config.py              # Paths and probe configs
â”‚   â”œâ”€â”€ metrics.py             # Spearman / AUC evaluation
â”‚   â””â”€â”€ utils.py               # Data ingestion helpers
â”‚
â”œâ”€â”€ notebooks/                 # Analysis & routing notebooks
â”‚   â”œâ”€â”€ router_utils.py        #   Shared routing + plotting utilities
â”‚   â”œâ”€â”€ utility_router.ipynb   #   Î»-sweep utility router (Pareto analysis)
â”‚   â”œâ”€â”€ cascade.ipynb          #   Threshold-based cascade router
â”‚   â””â”€â”€ test_probe_library.ipynb
â”‚
â”œâ”€â”€ create_sr_datasets/        # Success-rate dataset generation (vLLM)
â”‚   â”œâ”€â”€ create_pareto_sr_datasets.py
â”‚   â”œâ”€â”€ create_code_pareto_sr_datasets.py
â”‚   â””â”€â”€ utils/                 #   Verification & evaluation helpers
â”‚
â”œâ”€â”€ setup.sh                   # One-shot env creation + editable install
â”œâ”€â”€ pika.yml                   # Conda environment spec
â”œâ”€â”€ pyproject.toml             # Package metadata & dependencies
â”œâ”€â”€ label_with_probe.sh        # Label a dataset split with a trained probe
â””â”€â”€ run_flexible_probe_sweep.sh # Sweep probes across models/datasets
```


## Supported Probes

| Probe | Description |
|---|---|
| `linear_eoi_probe` | Linear probe on end-of-input hidden states |
| `tfidf_probe` | TF-IDF features â†’ linear model |
| `attn_probe` | Attention-based probe (via `TorchProbe`) |
| `length_probe` | Token-length baseline |
| `linear_then_max_probe` | Linear â†’ max-pool architecture |
| `linear_then_softmax_probe` | Linear â†’ softmax-pool architecture |
| `linear_then_rolling_max_probe` | Linear â†’ rolling-max-pool architecture |

## Usage

### Train a probe

```bash
python src/pika/main.py \
    --probe linear_eoi_probe \
    --dataset DigitalLearningGmbH_MATH-lighteval \
    --model Qwen/Qwen2.5-Math-7B-Instruct \
    --max_len 3000 --k 50 --temperature 0.7
```

### Label data with a trained probe

```bash
bash label_with_probe.sh
```

### Run a probe sweep across models and datasets

```bash
bash run_flexible_probe_sweep.sh
```

### Routing analysis

Open the notebooks in `notebooks/`:

- **`utility_router.ipynb`** â€” Probe-based utility router with Î»-sweep, Pareto frontier visualisation, and LaTeX table generation.
- **`cascade.ipynb`** â€” Threshold-based cascade router that escalates to more capable models when probe confidence is low.

Both notebooks import shared functions from `notebooks/router_utils.py`.

## Benchmarks

The routing experiments support three benchmarks:

- [MATH](https://huggingface.co/datasets/DigitalLearningGmbH/MATH-lighteval) â€” competition mathematics
- [GSM8K](https://huggingface.co/datasets/openai/gsm8k) â€” grade school math
- [AIME](https://huggingface.co/datasets/gneubig/aime-1983-2024) â€” AMC/AIME problems

## ðŸ“š Resources

### Paper

- **arXiv (February 2026)**: [LLMs Encode Their Failures: Predicting Success from Pre-Generation Activations](https://arxiv.org/abs/2602.09924)
- **arXiv (October 2025)**: [LLMs Encode How Difficult Problems Are](https://arxiv.org/abs/2510.18147)

### Pre-trained Probes

All trained probes are available on HuggingFace Hub:

- **[CoffeeGitta/pika-probes](https://huggingface.co/CoffeeGitta/pika-probes)** â€” Pre-trained probes for MATH, GSM8K, and other datasets
  - Includes probes for GPT-OSS-20B (high/low/medium difficulty), Qwen2.5-Math-7B, and more
  - Each probe config includes layer and position metadata for easy inference

We continuously upload new probes as experiments complete. See the repo for the latest additions.

### Generation Datasets

Model generations on benchmark tasks are available on HuggingFace Hub:

- **[CoffeeGitta/pika-math-generations](https://huggingface.co/datasets/CoffeeGitta/pika-math-generations)** â€” MATH dataset generations
  - Configs for GPT-OSS-20B (high/low/medium difficulty)
  - Includes correctness annotations and generation hyperparameters
  - Train/validation/test splits

Additional dataset uploads are in progress. Check back for updates as we expand coverage.

## Citation

If you use this code, please cite:

```bibtex

@misc{lugoloobi_llms_2026,
	title = {{LLMs} {Encode} {Their} {Failures}: {Predicting} {Success} from {Pre}-{Generation} {Activations}},
	shorttitle = {{LLMs} {Encode} {Their} {Failures}},
	url = {http://arxiv.org/abs/2602.09924},
	doi = {10.48550/arXiv.2602.09924},
	abstract = {Running LLMs with extended reasoning on every problem is expensive, but determining which inputs actually require additional compute remains challenging. We investigate whether their own likelihood of success is recoverable from their internal representations before generation, and if this signal can guide more efficient inference. We train linear probes on pre-generation activations to predict policy-specific success on math and coding tasks, substantially outperforming surface features such as question length and TF-IDF. Using E2H-AMC, which provides both human and model performance on identical problems, we show that models encode a model-specific notion of difficulty that is distinct from human difficulty, and that this distinction increases with extended reasoning. Leveraging these probes, we demonstrate that routing queries across a pool of models can exceed the best-performing model whilst reducing inference cost by up to 70{\textbackslash}\% on MATH, showing that internal representations enable practical efficiency gains even when they diverge from human intuitions about difficulty. Our code is available at: https://github.com/KabakaWilliam/llms\_know\_difficulty},
	urldate = {2026-02-11},
	publisher = {arXiv},
	author = {Lugoloobi, William and Foster, Thomas and Bankes, William and Russell, Chris},
	month = feb,
	year = {2026},
	note = {arXiv:2602.09924 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}


@misc{lugoloobi_llms_2025,
    title = {{LLMs} {Encode} {How} {Difficult} {Problems} {Are}},
    url = {http://arxiv.org/abs/2510.18147},
    doi = {10.48550/arXiv.2510.18147},
    publisher = {arXiv},
    author = {Lugoloobi, William and Russell, Chris},
    month = oct,
    year = {2025},
    note = {arXiv:2510.18147 [cs]},
}
```

## License
MIT
