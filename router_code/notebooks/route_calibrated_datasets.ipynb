{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4c547c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import gc\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Any, List, Optional\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from vllm import LLM, SamplingParams\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "repo_root = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(repo_root))\n",
    "\n",
    "from will_replication.my_utils.utils import load_probe_data, sigmoid_np, load_labelled_probe_dataset, SIMPLE_MODEL_POOL_CONFIG, ModelConfig\n",
    "from thom_replication.utils.verification_math import compute_score, extract_solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc9d08ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROBING CONGIG CSETUP\n",
    "PROBE_RESULTS_DIR = \"../../will_replication/probe_results/DATA\"\n",
    "LABELLED_SR_PATH = f\"{PROBE_RESULTS_DIR}/Labelled_SR\"\n",
    "PROBE_DATA_PATH=f\"{PROBE_RESULTS_DIR}/SR_DATA\"\n",
    "PROBING_DATASET = \"MATH\"\n",
    "\n",
    "LABELLED_DATASET_FULL_NAME = \"gneubig/aime-1983-2024\"\n",
    "LABELLED_DATASET_NAME = \"_\".join(LABELLED_DATASET_FULL_NAME.split(\"/\"))\n",
    "\n",
    "PROBE_MODEL_NAME = \"Qwen/Qwen2.5-Math-1.5B-Instruct\"\n",
    "MODEL_ALIAS = \"-\".join(PROBE_MODEL_NAME.split(\"/\"))\n",
    "K=1\n",
    "TEMPERATURE=0.0\n",
    "\n",
    "\n",
    "small_model_probe_data = load_probe_data(MODEL_NAME=PROBE_MODEL_NAME, PROBING_DATASET=PROBING_DATASET, K=K, TEMPERATURE=TEMPERATURE, DATA_PATH=PROBE_DATA_PATH)\n",
    "\n",
    "labelled_datast_df = load_labelled_probe_dataset(MODEL_NAME=PROBE_MODEL_NAME, PROBE_SOURCE_DATASET=PROBING_DATASET, LABELLED_DATASET=LABELLED_DATASET_NAME, K=K, TEMPERATURE=TEMPERATURE, DATA_PATH=LABELLED_SR_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c1e91c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "routing_dataset_df = labelled_datast_df.copy().sample(n=100, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c20666",
   "metadata": {},
   "source": [
    "# Route questions to a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7f26591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Qwen/Qwen2.5-Math-1.5B-Instruct',\n",
       " 'Qwen/Qwen2.5-Math-7B-Instruct',\n",
       " 'Qwen/Qwen2.5-Math-72B-Instruct']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_POOL = list(SIMPLE_MODEL_POOL_CONFIG.keys())\n",
    "MODEL_POOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef6e213c",
   "metadata": {},
   "outputs": [],
   "source": [
    "routing_dataset_df[\"route_to\"] = PROBE_MODEL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ae15a0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>dataset</th>\n",
       "      <th>prompt_scored</th>\n",
       "      <th>formatted</th>\n",
       "      <th>score_raw</th>\n",
       "      <th>score</th>\n",
       "      <th>layer</th>\n",
       "      <th>pos</th>\n",
       "      <th>original_solution</th>\n",
       "      <th>Year</th>\n",
       "      <th>Problem Number</th>\n",
       "      <th>route_to</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>830</td>\n",
       "      <td>gneubig/aime-1983-2024</td>\n",
       "      <td>Zou and Chou are practicing their $100$ -meter...</td>\n",
       "      <td>&lt;|im_start|&gt;system\\nPlease reason step by step...</td>\n",
       "      <td>0.956127</td>\n",
       "      <td>0.722346</td>\n",
       "      <td>18</td>\n",
       "      <td>-1</td>\n",
       "      <td>97</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>Qwen/Qwen2.5-Math-1.5B-Instruct</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     idx                 dataset  \\\n",
       "830  830  gneubig/aime-1983-2024   \n",
       "\n",
       "                                         prompt_scored  \\\n",
       "830  Zou and Chou are practicing their $100$ -meter...   \n",
       "\n",
       "                                             formatted  score_raw     score  \\\n",
       "830  <|im_start|>system\\nPlease reason step by step...   0.956127  0.722346   \n",
       "\n",
       "     layer  pos original_solution  Year  Problem Number  \\\n",
       "830     18   -1                97  2021               1   \n",
       "\n",
       "                            route_to  \n",
       "830  Qwen/Qwen2.5-Math-1.5B-Instruct  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "routing_dataset_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca91dea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_questions(predicted_score:int, model_pool:list[str]):\n",
    "    if predicted_score  >= 0.8: #really easy go to model 1 (0.8 - 1.0)\n",
    "        return model_pool[0]\n",
    "    elif predicted_score >= 0.5: #medium go to model 2  (0.5 - 0.8)\n",
    "        return model_pool[1]\n",
    "    else:\n",
    "        return model_pool[2] #realy hard go to model 3 (0 - 0.49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f56bfbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "routing_dataset_df[\"route_to\"] = routing_dataset_df[\"score\"].apply(lambda x: route_questions(x, MODEL_POOL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "512a17eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "route_to\n",
       "Qwen/Qwen2.5-Math-72B-Instruct     66\n",
       "Qwen/Qwen2.5-Math-7B-Instruct      25\n",
       "Qwen/Qwen2.5-Math-1.5B-Instruct     9\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "routing_dataset_df[\"route_to\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2607e494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of DataFrames, each subset grouped by 'route_to'\n",
    "route_to_subsets = {route: group for route, group in routing_dataset_df.groupby('route_to')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfbdb9a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Qwen/Qwen2.5-Math-1.5B-Instruct',\n",
       " 'Qwen/Qwen2.5-Math-72B-Instruct',\n",
       " 'Qwen/Qwen2.5-Math-7B-Instruct']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(route_to_subsets.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35d21303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 66, 25)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(route_to_subsets[list(route_to_subsets.keys())[0]]), len(route_to_subsets[list(route_to_subsets.keys())[1]]), len(route_to_subsets[list(route_to_subsets.keys())[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f47ad6c",
   "metadata": {},
   "source": [
    "# Answer each question with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98f2ff7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,3\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba96908d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class VLLMModelRunCfg:\n",
    "    tensor_parallel_size: int = 1\n",
    "    gpu_memory_utilization: float = 0.90\n",
    "    max_model_len: int = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f68f54e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = SamplingParams(temperature=TEMPERATURE, n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1b0a981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unload_model(llm: LLM) -> None:\n",
    "    try:\n",
    "        if hasattr(llm, \"llm_engine\"):\n",
    "            del llm.llm_engine\n",
    "    except Exception:\n",
    "        pass\n",
    "    del llm\n",
    "    gc.collect()\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f445c74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zou and Chou are practicing their $100$ -meter sprints by running $6$ races against each other. Zou wins the first race, and after that, the probability that one of them wins a race is $\\frac23$ if they won the previous race but only $\\frac13$ if they lost the previous race. The probability that Zou will win exactly $5$ of the $6$ races is $\\frac mn$ , where $m$ and $n$ are relatively prime positive integers. Find $m+n$ . Let's think step by step and output the final answer within \\boxed{}.\n"
     ]
    }
   ],
   "source": [
    "print(routing_dataset_df[\"prompt_scored\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e7fcbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_apply_chat_template(problems, tokenizer):\n",
    "    prompt_store = []\n",
    "    for problem in problems:\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": problem}\n",
    "        ]\n",
    "        prompts = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        prompt_store.append(prompts)\n",
    "    return prompt_store\n",
    "\n",
    "def count_input_tokens_batch(prompts: list[str], tokenizer) -> list[int]:\n",
    "    enc = tokenizer(prompts, add_special_tokens=False)\n",
    "    # HF returns dict with \"input_ids\": List[List[int]]\n",
    "    return [len(ids) for ids in enc[\"input_ids\"]]\n",
    "\n",
    "TOKENS_PER_MILLION = 1_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19d85804",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_routed_vllm_inference(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    route_col: str,\n",
    "    prompt_col: str,\n",
    "    out_text_col: str = \"response_text\",\n",
    "    out_model_col: str = \"response_model\",\n",
    "    input_num_tokens_col: str = \"input_num_tokens\",\n",
    "    out_tok_col: str = \"response_num_tokens\",\n",
    "    out_latency_col: str = \"response_latency_s\",\n",
    "    out_err_col: str = \"response_error\",\n",
    "    input_cost_col: str = \"input_cost_usd\",\n",
    "    output_cost_col: str = \"output_cost_usd\",\n",
    "    total_cost_col: str = \"total_cost_usd\",\n",
    "    pricing_config: Optional[dict] = None,\n",
    "    temperature: float = 0.0,\n",
    "    max_tokens: int = 3000,\n",
    "    n: int = 1,\n",
    "    batch_size: int = 32,\n",
    "    checkpoint_path: Optional[str] = None,\n",
    "    model_run_cfgs: Optional[Dict[str, VLLMModelRunCfg]] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Runs vLLM inference grouped by df[route_col], and writes results back into df.\n",
    "    Safe to resume if out_text_col already filled.\n",
    "    \"\"\"\n",
    "\n",
    "    if model_run_cfgs is None:\n",
    "        model_run_cfgs = {}\n",
    "\n",
    "    if pricing_config is None:\n",
    "        # pass SIMPLE_MODEL_POOL_CONFIG here when you call the function\n",
    "        pricing_config = {}\n",
    "\n",
    "    # Ensure output columns exist\n",
    "    for col, default in [\n",
    "        (out_text_col, None),\n",
    "        (out_model_col, None),\n",
    "        (input_num_tokens_col, np.nan),\n",
    "        (out_tok_col, np.nan),\n",
    "        (out_latency_col, np.nan),\n",
    "        (out_err_col, None),\n",
    "        (input_cost_col, np.nan),\n",
    "        (output_cost_col, np.nan),\n",
    "        (total_cost_col, np.nan),\n",
    "    ]:\n",
    "        if col not in df.columns:\n",
    "            df[col] = default\n",
    "\n",
    "    # Only process rows without outputs yet\n",
    "    pending_mask = df[out_text_col].isna()\n",
    "    if pending_mask.sum() == 0:\n",
    "        print(\"✅ Nothing to do: all rows already have responses.\")\n",
    "        return df\n",
    "\n",
    "    routes = df.loc[pending_mask, route_col].dropna().unique().tolist()\n",
    "    print(f\"Routes to run: {routes}\")\n",
    "\n",
    "    for model_name in routes:\n",
    "        model_mask = pending_mask & (df[route_col] == model_name)\n",
    "        idxs = df.index[model_mask].tolist()\n",
    "        if not idxs:\n",
    "            continue\n",
    "\n",
    "        cfg = model_run_cfgs.get(model_name, VLLMModelRunCfg())\n",
    "        print(f\"\\n=== Running model: {model_name} | rows: {len(idxs)} ===\")\n",
    "        print(f\"vLLM cfg: {cfg}\")\n",
    "\n",
    "        # Pull pricing for this model (if available)\n",
    "        model_costs = pricing_config.get(model_name, {}).get(\"model_costs\", {})\n",
    "        in_rate = model_costs.get(\"input_per_mill\", None)   # USD per 1,000,000 input tokens\n",
    "        out_rate = model_costs.get(\"output_per_mill\", None) # USD per 1,000,000 output tokens\n",
    "        has_pricing = (in_rate is not None) and (out_rate is not None)\n",
    "        if not has_pricing:\n",
    "            print(f\"⚠️ No pricing found for {model_name} in pricing_config[...]['model_costs']; costs will be NaN.\")\n",
    "\n",
    "        if \"72\" in model_name:\n",
    "            llm = LLM(\n",
    "                model=model_name,\n",
    "                tensor_parallel_size=cfg.tensor_parallel_size,\n",
    "                gpu_memory_utilization=cfg.gpu_memory_utilization,\n",
    "                max_model_len=cfg.max_model_len,\n",
    "                max_num_seqs=64,\n",
    "                max_num_batched_tokens=8192,\n",
    "                # enforce_eager=True\n",
    "            )\n",
    "        else:\n",
    "            llm = LLM(\n",
    "                model=model_name,\n",
    "                tensor_parallel_size=cfg.tensor_parallel_size,\n",
    "                gpu_memory_utilization=cfg.gpu_memory_utilization,\n",
    "                max_model_len=cfg.max_model_len,\n",
    "            )\n",
    "\n",
    "        sampling = SamplingParams(\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            n=n,\n",
    "        )\n",
    "\n",
    "        tokenizer = llm.llm_engine.tokenizer.tokenizer\n",
    "\n",
    "\n",
    "        # Process in batches\n",
    "        for start in tqdm(range(0, len(idxs), batch_size), desc=f\"Inferencing {model_name}\"):\n",
    "            batch_idxs = idxs[start : start + batch_size]\n",
    "            problems = df.loc[batch_idxs, prompt_col].tolist()\n",
    "\n",
    "            prompts = batch_apply_chat_template(problems, tokenizer)\n",
    "            input_tok_counts = count_input_tokens_batch(prompts, tokenizer)\n",
    "\n",
    "\n",
    "            t0 = time.time()\n",
    "            try:\n",
    "                outputs = llm.generate(prompts, sampling_params=sampling)\n",
    "                latency = time.time() - t0\n",
    "\n",
    "                # vLLM returns outputs aligned with prompts\n",
    "                texts = []\n",
    "                out_tok_counts = []\n",
    "                errs = [None] * len(outputs)\n",
    "\n",
    "                for out in outputs:\n",
    "                    # If n>1 you might want list; here we take first completion by default\n",
    "                    comp = out.outputs[0]\n",
    "                    texts.append(comp.text)\n",
    "                    out_tok_counts.append(len(comp.token_ids) if comp.token_ids is not None else np.nan)\n",
    "\n",
    "                df.loc[batch_idxs, out_text_col] = texts\n",
    "                df.loc[batch_idxs, out_model_col] = model_name\n",
    "                df.loc[batch_idxs, input_num_tokens_col] = input_tok_counts\n",
    "                df.loc[batch_idxs, out_tok_col] = out_tok_counts\n",
    "                df.loc[batch_idxs, out_latency_col] = latency\n",
    "                df.loc[batch_idxs, out_err_col] = errs\n",
    "\n",
    "                # Compute + write costs (vectorized on the batch)\n",
    "                if has_pricing:\n",
    "                    in_arr = np.array(input_tok_counts, dtype=float)\n",
    "                    out_arr = np.array(out_tok_counts, dtype=float)\n",
    "\n",
    "                    input_costs = (in_arr / TOKENS_PER_MILLION) * float(in_rate)\n",
    "                    output_costs = (out_arr / TOKENS_PER_MILLION) * float(out_rate)\n",
    "                    total_costs = input_costs + output_costs\n",
    "\n",
    "                    df.loc[batch_idxs, input_cost_col] = input_costs\n",
    "                    df.loc[batch_idxs, output_cost_col] = output_costs\n",
    "                    df.loc[batch_idxs, total_cost_col] = total_costs\n",
    "                else:\n",
    "                    df.loc[batch_idxs, input_cost_col] = np.nan\n",
    "                    df.loc[batch_idxs, output_cost_col] = np.nan\n",
    "                    df.loc[batch_idxs, total_cost_col] = np.nan\n",
    "\n",
    "            except Exception as e:\n",
    "                latency = time.time() - t0\n",
    "                # record error per-row so you can retry later\n",
    "                df.loc[batch_idxs, out_text_col] = None\n",
    "                df.loc[batch_idxs, out_model_col] = model_name\n",
    "                df.loc[batch_idxs, input_num_tokens_col] = np.nan\n",
    "                df.loc[batch_idxs, out_tok_col] = np.nan\n",
    "                df.loc[batch_idxs, out_latency_col] = latency\n",
    "                df.loc[batch_idxs, out_err_col] = repr(e)\n",
    "\n",
    "                # also blank costs on error\n",
    "                df.loc[batch_idxs, input_cost_col] = np.nan\n",
    "                df.loc[batch_idxs, output_cost_col] = np.nan\n",
    "                df.loc[batch_idxs, total_cost_col] = np.nan\n",
    "                \n",
    "            # checkpoint frequently (optional)\n",
    "            if checkpoint_path is not None:\n",
    "                df.to_parquet(checkpoint_path, index=True)\n",
    "\n",
    "        unload_model(llm)\n",
    "\n",
    "        # refresh pending_mask for the next model\n",
    "        pending_mask = df[out_text_col].isna()\n",
    "\n",
    "        if checkpoint_path is not None:\n",
    "            df.to_parquet(checkpoint_path, index=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52bbbdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_run_cfgs = {\n",
    "    \"Qwen/Qwen2.5-Math-1.5B-Instruct\": VLLMModelRunCfg(tensor_parallel_size=1, gpu_memory_utilization=0.60, max_model_len=4096),\n",
    "    \"Qwen/Qwen2.5-Math-7B-Instruct\":   VLLMModelRunCfg(tensor_parallel_size=1, gpu_memory_utilization=0.70, max_model_len=4096),\n",
    "    \"Qwen/Qwen2.5-Math-72B-Instruct\":  VLLMModelRunCfg(tensor_parallel_size=2, gpu_memory_utilization=0.92, max_model_len=4096),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c13f7f21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>dataset</th>\n",
       "      <th>prompt_scored</th>\n",
       "      <th>formatted</th>\n",
       "      <th>score_raw</th>\n",
       "      <th>score</th>\n",
       "      <th>layer</th>\n",
       "      <th>pos</th>\n",
       "      <th>original_solution</th>\n",
       "      <th>Year</th>\n",
       "      <th>Problem Number</th>\n",
       "      <th>route_to</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>830</td>\n",
       "      <td>gneubig/aime-1983-2024</td>\n",
       "      <td>Zou and Chou are practicing their $100$ -meter...</td>\n",
       "      <td>&lt;|im_start|&gt;system\\nPlease reason step by step...</td>\n",
       "      <td>0.956127</td>\n",
       "      <td>0.722346</td>\n",
       "      <td>18</td>\n",
       "      <td>-1</td>\n",
       "      <td>97</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>Qwen/Qwen2.5-Math-7B-Instruct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>70</td>\n",
       "      <td>gneubig/aime-1983-2024</td>\n",
       "      <td>Compute \\[\\frac{(10^4+324)(22^4+324)(34^4+324)...</td>\n",
       "      <td>&lt;|im_start|&gt;system\\nPlease reason step by step...</td>\n",
       "      <td>-0.575689</td>\n",
       "      <td>0.359925</td>\n",
       "      <td>18</td>\n",
       "      <td>-1</td>\n",
       "      <td>373</td>\n",
       "      <td>1987</td>\n",
       "      <td>14</td>\n",
       "      <td>Qwen/Qwen2.5-Math-72B-Instruct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>631</td>\n",
       "      <td>gneubig/aime-1983-2024</td>\n",
       "      <td>Let $A=\\{1,2,3,4\\}$ , and $f$ and $g$ be rando...</td>\n",
       "      <td>&lt;|im_start|&gt;system\\nPlease reason step by step...</td>\n",
       "      <td>0.533384</td>\n",
       "      <td>0.630272</td>\n",
       "      <td>18</td>\n",
       "      <td>-1</td>\n",
       "      <td>453</td>\n",
       "      <td>2014</td>\n",
       "      <td>12</td>\n",
       "      <td>Qwen/Qwen2.5-Math-7B-Instruct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>506</td>\n",
       "      <td>gneubig/aime-1983-2024</td>\n",
       "      <td>The sequence $(a_n)$ satisfies $a_0=0$ and $a_...</td>\n",
       "      <td>&lt;|im_start|&gt;system\\nPlease reason step by step...</td>\n",
       "      <td>-0.070733</td>\n",
       "      <td>0.482324</td>\n",
       "      <td>18</td>\n",
       "      <td>-1</td>\n",
       "      <td>983</td>\n",
       "      <td>2009</td>\n",
       "      <td>14</td>\n",
       "      <td>Qwen/Qwen2.5-Math-72B-Instruct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704</th>\n",
       "      <td>704</td>\n",
       "      <td>gneubig/aime-1983-2024</td>\n",
       "      <td>Triangle $ABC$ is inscribed in circle $\\omega$...</td>\n",
       "      <td>&lt;|im_start|&gt;system\\nPlease reason step by step...</td>\n",
       "      <td>-1.206015</td>\n",
       "      <td>0.230407</td>\n",
       "      <td>18</td>\n",
       "      <td>-1</td>\n",
       "      <td>43</td>\n",
       "      <td>2016</td>\n",
       "      <td>10</td>\n",
       "      <td>Qwen/Qwen2.5-Math-72B-Instruct</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     idx                 dataset  \\\n",
       "830  830  gneubig/aime-1983-2024   \n",
       "70    70  gneubig/aime-1983-2024   \n",
       "631  631  gneubig/aime-1983-2024   \n",
       "506  506  gneubig/aime-1983-2024   \n",
       "704  704  gneubig/aime-1983-2024   \n",
       "\n",
       "                                         prompt_scored  \\\n",
       "830  Zou and Chou are practicing their $100$ -meter...   \n",
       "70   Compute \\[\\frac{(10^4+324)(22^4+324)(34^4+324)...   \n",
       "631  Let $A=\\{1,2,3,4\\}$ , and $f$ and $g$ be rando...   \n",
       "506  The sequence $(a_n)$ satisfies $a_0=0$ and $a_...   \n",
       "704  Triangle $ABC$ is inscribed in circle $\\omega$...   \n",
       "\n",
       "                                             formatted  score_raw     score  \\\n",
       "830  <|im_start|>system\\nPlease reason step by step...   0.956127  0.722346   \n",
       "70   <|im_start|>system\\nPlease reason step by step...  -0.575689  0.359925   \n",
       "631  <|im_start|>system\\nPlease reason step by step...   0.533384  0.630272   \n",
       "506  <|im_start|>system\\nPlease reason step by step...  -0.070733  0.482324   \n",
       "704  <|im_start|>system\\nPlease reason step by step...  -1.206015  0.230407   \n",
       "\n",
       "     layer  pos original_solution  Year  Problem Number  \\\n",
       "830     18   -1                97  2021               1   \n",
       "70      18   -1               373  1987              14   \n",
       "631     18   -1               453  2014              12   \n",
       "506     18   -1               983  2009              14   \n",
       "704     18   -1                43  2016              10   \n",
       "\n",
       "                           route_to  \n",
       "830   Qwen/Qwen2.5-Math-7B-Instruct  \n",
       "70   Qwen/Qwen2.5-Math-72B-Instruct  \n",
       "631   Qwen/Qwen2.5-Math-7B-Instruct  \n",
       "506  Qwen/Qwen2.5-Math-72B-Instruct  \n",
       "704  Qwen/Qwen2.5-Math-72B-Instruct  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "routing_dataset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "193de557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Routes to run: ['Qwen/Qwen2.5-Math-7B-Instruct', 'Qwen/Qwen2.5-Math-72B-Instruct', 'Qwen/Qwen2.5-Math-1.5B-Instruct']\n",
      "\n",
      "=== Running model: Qwen/Qwen2.5-Math-7B-Instruct | rows: 25 ===\n",
      "vLLM cfg: VLLMModelRunCfg(tensor_parallel_size=1, gpu_memory_utilization=0.7, max_model_len=4096)\n",
      "INFO 12-26 19:51:49 [utils.py:326] non-default args: {'model': 'Qwen/Qwen2.5-Math-7B-Instruct', 'max_model_len': 4096, 'gpu_memory_utilization': 0.7, 'disable_log_stats': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-26 19:51:58 [__init__.py:711] Resolved architecture: Qwen2ForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-26 19:51:59 [__init__.py:1750] Using max model len 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-26 19:52:00,356\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-26 19:52:00 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "\u001b[1;36m(EngineCore_0 pid=3059686)\u001b[0;0m INFO 12-26 19:52:01 [core.py:636] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_0 pid=3059686)\u001b[0;0m INFO 12-26 19:52:01 [core.py:74] Initializing a V1 LLM engine (v0.10.1.1) with config: model='Qwen/Qwen2.5-Math-7B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-Math-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen2.5-Math-7B-Instruct, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_0 pid=3059686)\u001b[0;0m INFO 12-26 19:52:06 [parallel_state.py:1134] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_0 pid=3059686)\u001b[0;0m WARNING 12-26 19:52:06 [topk_topp_sampler.py:61] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_0 pid=3059686)\u001b[0;0m INFO 12-26 19:52:06 [gpu_model_runner.py:1953] Starting to load model Qwen/Qwen2.5-Math-7B-Instruct...\n",
      "\u001b[1;36m(EngineCore_0 pid=3059686)\u001b[0;0m INFO 12-26 19:52:06 [gpu_model_runner.py:1985] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_0 pid=3059686)\u001b[0;0m INFO 12-26 19:52:07 [cuda.py:328] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(EngineCore_0 pid=3059686)\u001b[0;0m INFO 12-26 19:52:07 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  2.05it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  2.01it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.95it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.91it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.94it/s]\n",
      "\u001b[1;36m(EngineCore_0 pid=3059686)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_0 pid=3059686)\u001b[0;0m INFO 12-26 19:52:09 [default_loader.py:262] Loading weights took 2.12 seconds\n",
      "\u001b[1;36m(EngineCore_0 pid=3059686)\u001b[0;0m INFO 12-26 19:52:10 [gpu_model_runner.py:2007] Model loading took 14.2419 GiB and 2.782169 seconds\n",
      "\u001b[1;36m(EngineCore_0 pid=3059686)\u001b[0;0m INFO 12-26 19:52:15 [backends.py:548] Using cache directory: /home/lina4335/.cache/vllm/torch_compile_cache/4858120f0f/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_0 pid=3059686)\u001b[0;0m INFO 12-26 19:52:15 [backends.py:559] Dynamo bytecode transform time: 5.47 s\n",
      "\u001b[1;36m(EngineCore_0 pid=3059686)\u001b[0;0m INFO 12-26 19:52:19 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 3.199 s\n",
      "\u001b[1;36m(EngineCore_0 pid=3059686)\u001b[0;0m INFO 12-26 19:52:19 [monitor.py:34] torch.compile takes 5.47 s in total\n",
      "\u001b[1;36m(EngineCore_0 pid=3059686)\u001b[0;0m INFO 12-26 19:52:20 [gpu_worker.py:276] Available KV cache memory: 35.47 GiB\n",
      "\u001b[1;36m(EngineCore_0 pid=3059686)\u001b[0;0m INFO 12-26 19:52:21 [kv_cache_utils.py:849] GPU KV cache size: 664,144 tokens\n",
      "\u001b[1;36m(EngineCore_0 pid=3059686)\u001b[0;0m INFO 12-26 19:52:21 [kv_cache_utils.py:853] Maximum concurrency for 4,096 tokens per request: 162.14x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:01<00:00, 37.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_0 pid=3059686)\u001b[0;0m INFO 12-26 19:52:23 [gpu_model_runner.py:2708] Graph capturing finished in 2 secs, took 0.64 GiB\n",
      "\u001b[1;36m(EngineCore_0 pid=3059686)\u001b[0;0m INFO 12-26 19:52:23 [core.py:214] init engine (profile, create kv cache, warmup model) took 12.93 seconds\n",
      "INFO 12-26 19:52:23 [llm.py:298] Supported_tasks: ['generate']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 16/16 [00:00<00:00, 2175.89it/s]0:00<?, ?it/s]\n",
      "Processed prompts: 100%|██████████| 16/16 [00:20<00:00,  1.29s/it, est. speed input: 112.31 toks/s, output: 1017.09 toks/s]\n",
      "Adding requests: 100%|██████████| 9/9 [00:00<00:00, 2406.06it/s][00:20<00:20, 20.64s/it]\n",
      "Processed prompts: 100%|██████████| 9/9 [00:20<00:00,  2.24s/it, est. speed input: 57.88 toks/s, output: 605.29 toks/s]\n",
      "Inferencing Qwen/Qwen2.5-Math-7B-Instruct: 100%|██████████| 2/2 [00:40<00:00, 20.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running model: Qwen/Qwen2.5-Math-72B-Instruct | rows: 66 ===\n",
      "vLLM cfg: VLLMModelRunCfg(tensor_parallel_size=2, gpu_memory_utilization=0.92, max_model_len=4096)\n",
      "INFO 12-26 19:53:08 [utils.py:326] non-default args: {'model': 'Qwen/Qwen2.5-Math-72B-Instruct', 'max_model_len': 4096, 'tensor_parallel_size': 2, 'gpu_memory_utilization': 0.92, 'max_num_batched_tokens': 8192, 'max_num_seqs': 64, 'disable_log_stats': True}\n",
      "INFO 12-26 19:53:09 [__init__.py:711] Resolved architecture: Qwen2ForCausalLM\n",
      "INFO 12-26 19:53:09 [__init__.py:1750] Using max model len 4096\n",
      "INFO 12-26 19:53:10 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "WARNING 12-26 19:53:11 [__init__.py:2921] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
      "INFO 12-26 19:53:17 [__init__.py:241] Automatically detected platform cuda.\n",
      "\u001b[1;36m(EngineCore_0 pid=3062104)\u001b[0;0m INFO 12-26 19:53:19 [core.py:636] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_0 pid=3062104)\u001b[0;0m INFO 12-26 19:53:19 [core.py:74] Initializing a V1 LLM engine (v0.10.1.1) with config: model='Qwen/Qwen2.5-Math-72B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-Math-72B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen2.5-Math-72B-Instruct, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":128,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_0 pid=3062104)\u001b[0;0m WARNING 12-26 19:53:19 [multiproc_worker_utils.py:273] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "\u001b[1;36m(EngineCore_0 pid=3062104)\u001b[0;0m INFO 12-26 19:53:19 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 16777216, 10, 'psm_c5f82ee5'), local_subscribe_addr='ipc:///tmp/8f9b25c3-5340-44ab-b88f-8a2d5b669cba', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 12-26 19:53:23 [__init__.py:241] Automatically detected platform cuda.\n",
      "INFO 12-26 19:53:24 [__init__.py:241] Automatically detected platform cuda.\n",
      "\u001b[1;36m(VllmWorker TP0 pid=3062569)\u001b[0;0m INFO 12-26 19:53:28 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_6a4f9d14'), local_subscribe_addr='ipc:///tmp/cd568ce1-443e-44eb-bba1-350d380fb1d4', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker TP1 pid=3062570)\u001b[0;0m INFO 12-26 19:53:28 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_579fb65a'), local_subscribe_addr='ipc:///tmp/d82a025f-244b-447b-a113-f725061db2f7', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker TP0 pid=3062569)\u001b[0;0m INFO 12-26 19:53:29 [__init__.py:1418] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker TP1 pid=3062570)\u001b[0;0m INFO 12-26 19:53:29 [__init__.py:1418] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker TP0 pid=3062569)\u001b[0;0m INFO 12-26 19:53:29 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker TP1 pid=3062570)\u001b[0;0m INFO 12-26 19:53:29 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker TP0 pid=3062569)\u001b[0;0m INFO 12-26 19:53:31 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.\n",
      "\u001b[1;36m(VllmWorker TP1 pid=3062570)\u001b[0;0m INFO 12-26 19:53:31 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.\n",
      "\u001b[1;36m(VllmWorker TP0 pid=3062569)\u001b[0;0m INFO 12-26 19:53:31 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_dc2ec144'), local_subscribe_addr='ipc:///tmp/5dbf3d39-7138-492f-bfcb-6a662efcea4c', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker TP1 pid=3062570)\u001b[0;0m INFO 12-26 19:53:31 [parallel_state.py:1134] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(VllmWorker TP0 pid=3062569)\u001b[0;0m INFO 12-26 19:53:31 [parallel_state.py:1134] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker TP1 pid=3062570)\u001b[0;0m WARNING 12-26 19:53:31 [topk_topp_sampler.py:61] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker TP0 pid=3062569)\u001b[0;0m WARNING 12-26 19:53:31 [topk_topp_sampler.py:61] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker TP1 pid=3062570)\u001b[0;0m INFO 12-26 19:53:31 [gpu_model_runner.py:1953] Starting to load model Qwen/Qwen2.5-Math-72B-Instruct...\n",
      "\u001b[1;36m(VllmWorker TP0 pid=3062569)\u001b[0;0m INFO 12-26 19:53:31 [gpu_model_runner.py:1953] Starting to load model Qwen/Qwen2.5-Math-72B-Instruct...\n",
      "\u001b[1;36m(VllmWorker TP0 pid=3062569)\u001b[0;0m INFO 12-26 19:53:31 [gpu_model_runner.py:1985] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker TP1 pid=3062570)\u001b[0;0m INFO 12-26 19:53:31 [gpu_model_runner.py:1985] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker TP0 pid=3062569)\u001b[0;0m INFO 12-26 19:53:31 [cuda.py:328] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker TP1 pid=3062570)\u001b[0;0m INFO 12-26 19:53:31 [cuda.py:328] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker TP1 pid=3062570)\u001b[0;0m INFO 12-26 19:53:31 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker TP0 pid=3062569)\u001b[0;0m INFO 12-26 19:53:32 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/37 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:   3% Completed | 1/37 [00:00<00:22,  1.63it/s]\n",
      "Loading safetensors checkpoint shards:   5% Completed | 2/37 [00:01<00:20,  1.68it/s]\n",
      "Loading safetensors checkpoint shards:   8% Completed | 3/37 [00:01<00:19,  1.74it/s]\n",
      "Loading safetensors checkpoint shards:  11% Completed | 4/37 [00:02<00:20,  1.61it/s]\n",
      "Loading safetensors checkpoint shards:  14% Completed | 5/37 [00:03<00:20,  1.54it/s]\n",
      "Loading safetensors checkpoint shards:  16% Completed | 6/37 [00:03<00:19,  1.56it/s]\n",
      "Loading safetensors checkpoint shards:  19% Completed | 7/37 [00:04<00:18,  1.66it/s]\n",
      "Loading safetensors checkpoint shards:  22% Completed | 8/37 [00:04<00:16,  1.72it/s]\n",
      "Loading safetensors checkpoint shards:  24% Completed | 9/37 [00:05<00:16,  1.66it/s]\n",
      "Loading safetensors checkpoint shards:  27% Completed | 10/37 [00:05<00:14,  1.83it/s]\n",
      "Loading safetensors checkpoint shards:  30% Completed | 11/37 [00:06<00:14,  1.82it/s]\n",
      "Loading safetensors checkpoint shards:  32% Completed | 12/37 [00:06<00:13,  1.82it/s]\n",
      "Loading safetensors checkpoint shards:  35% Completed | 13/37 [00:07<00:13,  1.81it/s]\n",
      "Loading safetensors checkpoint shards:  38% Completed | 14/37 [00:08<00:12,  1.84it/s]\n",
      "Loading safetensors checkpoint shards:  41% Completed | 15/37 [00:08<00:11,  1.86it/s]\n",
      "Loading safetensors checkpoint shards:  43% Completed | 16/37 [00:09<00:11,  1.88it/s]\n",
      "Loading safetensors checkpoint shards:  46% Completed | 17/37 [00:09<00:10,  1.85it/s]\n",
      "Loading safetensors checkpoint shards:  49% Completed | 18/37 [00:10<00:11,  1.68it/s]\n",
      "Loading safetensors checkpoint shards:  51% Completed | 19/37 [00:11<00:10,  1.65it/s]\n",
      "Loading safetensors checkpoint shards:  54% Completed | 20/37 [00:11<00:09,  1.86it/s]\n",
      "Loading safetensors checkpoint shards:  57% Completed | 21/37 [00:12<00:08,  1.80it/s]\n",
      "Loading safetensors checkpoint shards:  59% Completed | 22/37 [00:12<00:08,  1.80it/s]\n",
      "Loading safetensors checkpoint shards:  62% Completed | 23/37 [00:13<00:07,  1.78it/s]\n",
      "Loading safetensors checkpoint shards:  65% Completed | 24/37 [00:13<00:07,  1.78it/s]\n",
      "Loading safetensors checkpoint shards:  68% Completed | 25/37 [00:14<00:06,  1.75it/s]\n",
      "Loading safetensors checkpoint shards:  70% Completed | 26/37 [00:14<00:06,  1.74it/s]\n",
      "Loading safetensors checkpoint shards:  73% Completed | 27/37 [00:15<00:05,  1.74it/s]\n",
      "Loading safetensors checkpoint shards:  76% Completed | 28/37 [00:16<00:05,  1.73it/s]\n",
      "Loading safetensors checkpoint shards:  78% Completed | 29/37 [00:16<00:04,  1.65it/s]\n",
      "Loading safetensors checkpoint shards:  81% Completed | 30/37 [00:17<00:04,  1.61it/s]\n",
      "Loading safetensors checkpoint shards:  84% Completed | 31/37 [00:18<00:03,  1.58it/s]\n",
      "Loading safetensors checkpoint shards:  86% Completed | 32/37 [00:18<00:03,  1.54it/s]\n",
      "Loading safetensors checkpoint shards:  89% Completed | 33/37 [00:19<00:02,  1.59it/s]\n",
      "Loading safetensors checkpoint shards:  92% Completed | 34/37 [00:19<00:01,  1.58it/s]\n",
      "Loading safetensors checkpoint shards:  95% Completed | 35/37 [00:20<00:01,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker TP1 pid=3062570)\u001b[0;0m INFO 12-26 19:53:53 [default_loader.py:262] Loading weights took 21.39 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  97% Completed | 36/37 [00:21<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker TP1 pid=3062570)\u001b[0;0m INFO 12-26 19:53:54 [gpu_model_runner.py:2007] Model loading took 67.7935 GiB and 22.137338 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 37/37 [00:21<00:00,  1.66it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 37/37 [00:21<00:00,  1.70it/s]\n",
      "\u001b[1;36m(VllmWorker TP0 pid=3062569)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker TP0 pid=3062569)\u001b[0;0m INFO 12-26 19:53:54 [default_loader.py:262] Loading weights took 21.76 seconds\n",
      "\u001b[1;36m(VllmWorker TP0 pid=3062569)\u001b[0;0m INFO 12-26 19:53:54 [gpu_model_runner.py:2007] Model loading took 67.7935 GiB and 22.839806 seconds\n",
      "\u001b[1;36m(VllmWorker TP1 pid=3062570)\u001b[0;0m INFO 12-26 19:54:06 [backends.py:548] Using cache directory: /home/lina4335/.cache/vllm/torch_compile_cache/9b2b1d4bcb/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker TP1 pid=3062570)\u001b[0;0m INFO 12-26 19:54:06 [backends.py:559] Dynamo bytecode transform time: 12.04 s\n",
      "\u001b[1;36m(VllmWorker TP0 pid=3062569)\u001b[0;0m INFO 12-26 19:54:07 [backends.py:548] Using cache directory: /home/lina4335/.cache/vllm/torch_compile_cache/9b2b1d4bcb/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker TP0 pid=3062569)\u001b[0;0m INFO 12-26 19:54:07 [backends.py:559] Dynamo bytecode transform time: 12.78 s\n",
      "\u001b[1;36m(VllmWorker TP1 pid=3062570)\u001b[0;0m INFO 12-26 19:54:16 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 9.150 s\n",
      "\u001b[1;36m(VllmWorker TP0 pid=3062569)\u001b[0;0m INFO 12-26 19:54:18 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 9.957 s\n",
      "\u001b[1;36m(VllmWorker TP1 pid=3062570)\u001b[0;0m INFO 12-26 19:54:19 [monitor.py:34] torch.compile takes 12.04 s in total\n",
      "\u001b[1;36m(VllmWorker TP0 pid=3062569)\u001b[0;0m INFO 12-26 19:54:19 [monitor.py:34] torch.compile takes 12.78 s in total\n",
      "\u001b[1;36m(VllmWorker TP0 pid=3062569)\u001b[0;0m INFO 12-26 19:54:21 [gpu_worker.py:276] Available KV cache memory: 3.19 GiB\n",
      "\u001b[1;36m(VllmWorker TP1 pid=3062570)\u001b[0;0m INFO 12-26 19:54:21 [gpu_worker.py:276] Available KV cache memory: 3.19 GiB\n",
      "\u001b[1;36m(EngineCore_0 pid=3062104)\u001b[0;0m INFO 12-26 19:54:21 [kv_cache_utils.py:849] GPU KV cache size: 20,896 tokens\n",
      "\u001b[1;36m(EngineCore_0 pid=3062104)\u001b[0;0m INFO 12-26 19:54:21 [kv_cache_utils.py:853] Maximum concurrency for 4,096 tokens per request: 5.10x\n",
      "\u001b[1;36m(EngineCore_0 pid=3062104)\u001b[0;0m INFO 12-26 19:54:21 [kv_cache_utils.py:849] GPU KV cache size: 20,896 tokens\n",
      "\u001b[1;36m(EngineCore_0 pid=3062104)\u001b[0;0m INFO 12-26 19:54:21 [kv_cache_utils.py:853] Maximum concurrency for 4,096 tokens per request: 5.10x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:01<00:00, 15.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker TP1 pid=3062570)\u001b[0;0m INFO 12-26 19:54:23 [custom_all_reduce.py:196] Registering 3059 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorker TP0 pid=3062569)\u001b[0;0m INFO 12-26 19:54:23 [custom_all_reduce.py:196] Registering 3059 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorker TP1 pid=3062570)\u001b[0;0m INFO 12-26 19:54:23 [gpu_model_runner.py:2708] Graph capturing finished in 2 secs, took 0.44 GiB\n",
      "\u001b[1;36m(VllmWorker TP0 pid=3062569)\u001b[0;0m INFO 12-26 19:54:23 [gpu_model_runner.py:2708] Graph capturing finished in 2 secs, took 0.44 GiB\n",
      "\u001b[1;36m(EngineCore_0 pid=3062104)\u001b[0;0m INFO 12-26 19:54:23 [core.py:214] init engine (profile, create kv cache, warmup model) took 28.76 seconds\n",
      "INFO 12-26 19:54:24 [llm.py:298] Supported_tasks: ['generate']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 16/16 [00:00<00:00, 2306.62it/s]00:00<?, ?it/s]\n",
      "Processed prompts: 100%|██████████| 16/16 [01:17<00:00,  4.83s/it, est. speed input: 31.27 toks/s, output: 220.83 toks/s]\n",
      "Adding requests: 100%|██████████| 16/16 [00:00<00:00, 2020.32it/s]01:17<05:09, 77.33s/it]\n",
      "Processed prompts: 100%|██████████| 16/16 [00:48<00:00,  3.01s/it, est. speed input: 62.48 toks/s, output: 285.15 toks/s]\n",
      "Adding requests: 100%|██████████| 16/16 [00:00<00:00, 3594.09it/s]02:05<03:00, 60.18s/it]\n",
      "Processed prompts: 100%|██████████| 16/16 [01:29<00:00,  5.58s/it, est. speed input: 25.98 toks/s, output: 231.79 toks/s]\n",
      "Adding requests: 100%|██████████| 16/16 [00:00<00:00, 3263.26it/s]03:34<02:26, 73.48s/it]\n",
      "Processed prompts: 100%|██████████| 16/16 [01:29<00:00,  5.57s/it, est. speed input: 30.69 toks/s, output: 213.89 toks/s]\n",
      "Adding requests: 100%|██████████| 2/2 [00:00<00:00, 2243.54it/s] [05:03<01:19, 79.68s/it]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:38<00:00, 19.15s/it, est. speed input: 6.06 toks/s, output: 62.64 toks/s]\n",
      "Inferencing Qwen/Qwen2.5-Math-72B-Instruct: 100%|██████████| 5/5 [05:42<00:00, 68.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker TP0 pid=3062569)\u001b[0;0m INFO 12-26 20:00:06 [multiproc_executor.py:520] Parent process exited, terminating worker\n",
      "\u001b[1;36m(VllmWorker TP1 pid=3062570)\u001b[0;0m INFO 12-26 20:00:06 [multiproc_executor.py:520] Parent process exited, terminating worker\n",
      "\n",
      "=== Running model: Qwen/Qwen2.5-Math-1.5B-Instruct | rows: 9 ===\n",
      "vLLM cfg: VLLMModelRunCfg(tensor_parallel_size=1, gpu_memory_utilization=0.6, max_model_len=4096)\n",
      "INFO 12-26 20:00:10 [utils.py:326] non-default args: {'model': 'Qwen/Qwen2.5-Math-1.5B-Instruct', 'max_model_len': 4096, 'gpu_memory_utilization': 0.6, 'disable_log_stats': True}\n",
      "INFO 12-26 20:00:11 [__init__.py:711] Resolved architecture: Qwen2ForCausalLM\n",
      "INFO 12-26 20:00:11 [__init__.py:1750] Using max model len 4096\n",
      "INFO 12-26 20:00:11 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "INFO 12-26 20:00:20 [__init__.py:241] Automatically detected platform cuda.\n",
      "\u001b[1;36m(EngineCore_0 pid=3075198)\u001b[0;0m INFO 12-26 20:00:22 [core.py:636] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_0 pid=3075198)\u001b[0;0m INFO 12-26 20:00:22 [core.py:74] Initializing a V1 LLM engine (v0.10.1.1) with config: model='Qwen/Qwen2.5-Math-1.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-Math-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen2.5-Math-1.5B-Instruct, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_0 pid=3075198)\u001b[0;0m INFO 12-26 20:00:25 [parallel_state.py:1134] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_0 pid=3075198)\u001b[0;0m WARNING 12-26 20:00:25 [topk_topp_sampler.py:61] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_0 pid=3075198)\u001b[0;0m INFO 12-26 20:00:25 [gpu_model_runner.py:1953] Starting to load model Qwen/Qwen2.5-Math-1.5B-Instruct...\n",
      "\u001b[1;36m(EngineCore_0 pid=3075198)\u001b[0;0m INFO 12-26 20:00:25 [gpu_model_runner.py:1985] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_0 pid=3075198)\u001b[0;0m INFO 12-26 20:00:25 [cuda.py:328] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(EngineCore_0 pid=3075198)\u001b[0;0m INFO 12-26 20:00:25 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(EngineCore_0 pid=3075198)\u001b[0;0m INFO 12-26 20:00:25 [weight_utils.py:349] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.07it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.07it/s]\n",
      "\u001b[1;36m(EngineCore_0 pid=3075198)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_0 pid=3075198)\u001b[0;0m INFO 12-26 20:00:26 [default_loader.py:262] Loading weights took 0.59 seconds\n",
      "\u001b[1;36m(EngineCore_0 pid=3075198)\u001b[0;0m INFO 12-26 20:00:26 [gpu_model_runner.py:2007] Model loading took 2.8798 GiB and 1.293867 seconds\n",
      "\u001b[1;36m(EngineCore_0 pid=3075198)\u001b[0;0m INFO 12-26 20:00:32 [backends.py:548] Using cache directory: /home/lina4335/.cache/vllm/torch_compile_cache/63465de3ab/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_0 pid=3075198)\u001b[0;0m INFO 12-26 20:00:32 [backends.py:559] Dynamo bytecode transform time: 5.14 s\n",
      "\u001b[1;36m(EngineCore_0 pid=3075198)\u001b[0;0m INFO 12-26 20:00:35 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 3.149 s\n",
      "\u001b[1;36m(EngineCore_0 pid=3075198)\u001b[0;0m INFO 12-26 20:00:36 [monitor.py:34] torch.compile takes 5.14 s in total\n",
      "\u001b[1;36m(EngineCore_0 pid=3075198)\u001b[0;0m INFO 12-26 20:00:36 [gpu_worker.py:276] Available KV cache memory: 38.98 GiB\n",
      "\u001b[1;36m(EngineCore_0 pid=3075198)\u001b[0;0m INFO 12-26 20:00:37 [kv_cache_utils.py:849] GPU KV cache size: 1,459,856 tokens\n",
      "\u001b[1;36m(EngineCore_0 pid=3075198)\u001b[0;0m INFO 12-26 20:00:37 [kv_cache_utils.py:853] Maximum concurrency for 4,096 tokens per request: 356.41x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:01<00:00, 48.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_0 pid=3075198)\u001b[0;0m INFO 12-26 20:00:38 [gpu_model_runner.py:2708] Graph capturing finished in 2 secs, took 0.58 GiB\n",
      "\u001b[1;36m(EngineCore_0 pid=3075198)\u001b[0;0m INFO 12-26 20:00:38 [core.py:214] init engine (profile, create kv cache, warmup model) took 11.87 seconds\n",
      "INFO 12-26 20:00:39 [llm.py:298] Supported_tasks: ['generate']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 9/9 [00:00<00:00, 2547.32it/s]1 [00:00<?, ?it/s]\n",
      "Processed prompts: 100%|██████████| 9/9 [00:03<00:00,  2.91it/s, est. speed input: 337.52 toks/s, output: 1898.57 toks/s]\n",
      "Inferencing Qwen/Qwen2.5-Math-1.5B-Instruct: 100%|██████████| 1/1 [00:03<00:00,  3.15s/it]\n",
      "[rank0]:[W1226 20:00:43.604052336 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    }
   ],
   "source": [
    "# 3) Run inference and write results back into the SAME df\n",
    "routing_dataset_df = run_routed_vllm_inference(\n",
    "    routing_dataset_df,\n",
    "    route_col=\"route_to\",\n",
    "    prompt_col=\"prompt_scored\",\n",
    "    out_text_col=\"routed_response_text\",\n",
    "    input_num_tokens_col='input_num_tokens',\n",
    "    temperature=TEMPERATURE,\n",
    "    max_tokens=3000,\n",
    "    batch_size=16,  # raise if stable, lower if OOM\n",
    "    checkpoint_path=\"aime_routed_outputs.parquet\",\n",
    "    pricing_config=SIMPLE_MODEL_POOL_CONFIG,\n",
    "    model_run_cfgs=model_run_cfgs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bf6db54",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROUTED_DF = pd.read_parquet(\"gneubig_aime-1983-2024_routed_final.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d30acacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.verification_math import compute_score, extract_solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b3c7d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROUTED_DF[\"routed_is_correct\"] = ROUTED_DF.apply(lambda row: compute_score(row[\"routed_response_text\"], row[\"original_solution\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8269b0ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3322615219721329"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ROUTED_DF[\"routed_is_correct\"].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diff-direction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
