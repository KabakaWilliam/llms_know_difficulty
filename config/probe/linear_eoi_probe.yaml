# Linear EOI Probe Configuration
# Non-torch probe with different parameter structure

#@package _global_

_target_: llms_know_difficulty.probe.linear_eoi_probe.LinearEoiProbe
model_name: "gpt2"
alpha_grid: [0, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]
batch_size: 16
max_length: 1024
probe_name: "linear_eoi_probe"

# Can be turned on for super quick e2e testing...
test_mode: false
test_sample_size: 128